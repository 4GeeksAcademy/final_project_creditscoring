{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f9c529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: MissForest in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.2.3)\n",
      "Requirement already satisfied: lazypredict in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.16)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (2.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (3.10.8)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (1.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (0.13.2)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (8.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (1.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (2.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (1.5.3)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (4.6.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (3.1.3)\n",
      "Requirement already satisfied: pytest-runner in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (6.0.1)\n",
      "Requirement already satisfied: mlflow>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (3.9.0)\n",
      "Requirement already satisfied: mlflow-skinny==3.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.9.0)\n",
      "Requirement already satisfied: mlflow-tracing==3.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.9.0)\n",
      "Requirement already satisfied: Flask-CORS<7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (6.0.2)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.1.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (1.18.3)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (46.0.4)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.4.3)\n",
      "Requirement already satisfied: huey<3,>=2.5.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (2.6.0)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (22.0.0)\n",
      "Requirement already satisfied: skops<1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (0.13.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (2.0.46)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.0.2)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (6.2.6)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.1.2)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.82.0)\n",
      "Requirement already satisfied: fastapi<1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.128.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.1.46)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (8.7.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.39.1)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (6.33.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.12.5)\n",
      "Requirement already satisfied: python-dotenv<2,>=0.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.2.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.32.5)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.5.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.15.0)\n",
      "Requirement already satisfied: uvicorn<1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.40.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow>=2.0.0->lazypredict) (1.3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from click->lazypredict) (0.4.6)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography<47,>=43.0.0->mlflow>=2.0.0->lazypredict) (2.0.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.48.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from docker<8,>=4.0.0->mlflow>=2.0.0->lazypredict) (311)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from docker<8,>=4.0.0->mlflow>=2.0.0->lazypredict) (2.6.3)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.0.4)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.1.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.9.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from graphene<4->mlflow>=2.0.0->lazypredict) (3.2.7)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from graphene<4->mlflow>=2.0.0->lazypredict) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from graphene<4->mlflow>=2.0.0->lazypredict) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (3.3.2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.60b1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->lazypredict) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->lazypredict) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.0.0->lazypredict) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2026.1.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->lazypredict) (3.6.0)\n",
      "Requirement already satisfied: prettytable>=3.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from skops<1->mlflow>=2.0.0->lazypredict) (3.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.0.0->lazypredict) (3.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.12.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from uvicorn<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow>=2.0.0->lazypredict) (3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from prettytable>=3.9->skops<1->mlflow>=2.0.0->lazypredict) (0.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install missingno MissForest lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32b58887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e514f",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "El DataFrame df se crea a partir de la tabla base_datos_pripal de la base de datos credit_scoring.db. El archivo no se incluye en el repositorio debido a su tamaño; la generación de la base de datos a partir del CSV original y la creación de la tabla se explica detalladamente en el notebook data-collection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edc06e",
   "metadata": {},
   "source": [
    "Descriptive analysis:\n",
    "In this part of the project, we begin exploring the dataset created from the initial information obtained from the LendingClub dataset (Kaggle). The objective of this stage is to describe and understand the structure of the data, the variables and their types, their distributions, skewness, and the presence of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8135836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este df existen 192309 filas y 157 columnas\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(r\"c:\\Users\\User\\Documents\\GITHUB\\final_project_creditscoring\\Data\\credit_scoring.db\")\n",
    "df = pd.read_sql(\"SELECT * FROM main_table\", conn)\n",
    "conn.close()\n",
    "n_rows,n_cols = df.shape\n",
    "\n",
    "print(f'En este df existen {n_rows} filas y {n_cols} columnas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70934a2f",
   "metadata": {},
   "source": [
    "With the analysis below, we can understand that the dataset contains a large number of numerical variables, along with several categorical features represented as object types. This initial inspection highlights the need for feature selection and type handling in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3437e64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype\n",
       "float64    118\n",
       "object      38\n",
       "int64        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_types = df.dtypes.reset_index().rename(\n",
    "    columns={'index': 'column_name', 0: 'dtype'}\n",
    ")\n",
    "\n",
    "cols_types['dtype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "827c4112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>member_id</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>term</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>grade</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>emp_title</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>emp_length</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>home_ownership</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>verification_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>issue_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>loan_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pymnt_plan</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>url</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>desc</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>purpose</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>title</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>zip_code</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>addr_state</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>earliest_cr_line</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>initial_list_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>last_pymnt_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>next_pymnt_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>last_credit_pull_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>application_type</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>verification_status_joint</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>sec_app_earliest_cr_line</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>hardship_flag</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>hardship_type</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>hardship_reason</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>hardship_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>hardship_start_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>hardship_end_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>payment_plan_start_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>hardship_loan_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>disbursement_method</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>debt_settlement_flag</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>debt_settlement_flag_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>settlement_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>settlement_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   column_name   dtype\n",
       "1                    member_id  object\n",
       "5                         term  object\n",
       "8                        grade  object\n",
       "9                    sub_grade  object\n",
       "10                   emp_title  object\n",
       "11                  emp_length  object\n",
       "12              home_ownership  object\n",
       "14         verification_status  object\n",
       "15                     issue_d  object\n",
       "16                 loan_status  object\n",
       "17                  pymnt_plan  object\n",
       "18                         url  object\n",
       "19                        desc  object\n",
       "20                     purpose  object\n",
       "21                       title  object\n",
       "22                    zip_code  object\n",
       "23                  addr_state  object\n",
       "26            earliest_cr_line  object\n",
       "37         initial_list_status  object\n",
       "47                last_pymnt_d  object\n",
       "49                next_pymnt_d  object\n",
       "50          last_credit_pull_d  object\n",
       "56            application_type  object\n",
       "59   verification_status_joint  object\n",
       "118   sec_app_earliest_cr_line  object\n",
       "128              hardship_flag  object\n",
       "129              hardship_type  object\n",
       "130            hardship_reason  object\n",
       "131            hardship_status  object\n",
       "134        hardship_start_date  object\n",
       "135          hardship_end_date  object\n",
       "136    payment_plan_start_date  object\n",
       "139       hardship_loan_status  object\n",
       "143        disbursement_method  object\n",
       "144       debt_settlement_flag  object\n",
       "145  debt_settlement_flag_date  object\n",
       "146          settlement_status  object\n",
       "147            settlement_date  object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_types[cols_types['dtype'] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c9a7d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               column_name   dtype\n",
      "                                 loan_amnt float64\n",
      "                               funded_amnt float64\n",
      "                           funded_amnt_inv float64\n",
      "                                  int_rate float64\n",
      "                               installment float64\n",
      "                                annual_inc float64\n",
      "                                       dti float64\n",
      "                               delinq_2yrs float64\n",
      "                            fico_range_low float64\n",
      "                           fico_range_high float64\n",
      "                            inq_last_6mths float64\n",
      "                    mths_since_last_delinq float64\n",
      "                    mths_since_last_record float64\n",
      "                                  open_acc float64\n",
      "                                   pub_rec float64\n",
      "                                 revol_bal float64\n",
      "                                revol_util float64\n",
      "                                 total_acc float64\n",
      "                                 out_prncp float64\n",
      "                             out_prncp_inv float64\n",
      "                               total_pymnt float64\n",
      "                           total_pymnt_inv float64\n",
      "                           total_rec_prncp float64\n",
      "                             total_rec_int float64\n",
      "                        total_rec_late_fee float64\n",
      "                                recoveries float64\n",
      "                   collection_recovery_fee float64\n",
      "                           last_pymnt_amnt float64\n",
      "                      last_fico_range_high float64\n",
      "                       last_fico_range_low float64\n",
      "                collections_12_mths_ex_med float64\n",
      "               mths_since_last_major_derog float64\n",
      "                               policy_code float64\n",
      "                          annual_inc_joint float64\n",
      "                                 dti_joint float64\n",
      "                            acc_now_delinq float64\n",
      "                              tot_coll_amt float64\n",
      "                               tot_cur_bal float64\n",
      "                               open_acc_6m float64\n",
      "                               open_act_il float64\n",
      "                               open_il_12m float64\n",
      "                               open_il_24m float64\n",
      "                        mths_since_rcnt_il float64\n",
      "                              total_bal_il float64\n",
      "                                   il_util float64\n",
      "                               open_rv_12m float64\n",
      "                               open_rv_24m float64\n",
      "                                max_bal_bc float64\n",
      "                                  all_util float64\n",
      "                          total_rev_hi_lim float64\n",
      "                                    inq_fi float64\n",
      "                               total_cu_tl float64\n",
      "                              inq_last_12m float64\n",
      "                      acc_open_past_24mths float64\n",
      "                               avg_cur_bal float64\n",
      "                            bc_open_to_buy float64\n",
      "                                   bc_util float64\n",
      "                  chargeoff_within_12_mths float64\n",
      "                               delinq_amnt float64\n",
      "                        mo_sin_old_il_acct float64\n",
      "                      mo_sin_old_rev_tl_op float64\n",
      "                     mo_sin_rcnt_rev_tl_op float64\n",
      "                            mo_sin_rcnt_tl float64\n",
      "                                  mort_acc float64\n",
      "                      mths_since_recent_bc float64\n",
      "                  mths_since_recent_bc_dlq float64\n",
      "                     mths_since_recent_inq float64\n",
      "            mths_since_recent_revol_delinq float64\n",
      "                     num_accts_ever_120_pd float64\n",
      "                            num_actv_bc_tl float64\n",
      "                           num_actv_rev_tl float64\n",
      "                               num_bc_sats float64\n",
      "                                 num_bc_tl float64\n",
      "                                 num_il_tl float64\n",
      "                             num_op_rev_tl float64\n",
      "                             num_rev_accts float64\n",
      "                       num_rev_tl_bal_gt_0 float64\n",
      "                                  num_sats float64\n",
      "                          num_tl_120dpd_2m float64\n",
      "                              num_tl_30dpd float64\n",
      "                        num_tl_90g_dpd_24m float64\n",
      "                        num_tl_op_past_12m float64\n",
      "                            pct_tl_nvr_dlq float64\n",
      "                          percent_bc_gt_75 float64\n",
      "                      pub_rec_bankruptcies float64\n",
      "                                 tax_liens float64\n",
      "                           tot_hi_cred_lim float64\n",
      "                         total_bal_ex_mort float64\n",
      "                            total_bc_limit float64\n",
      "                total_il_high_credit_limit float64\n",
      "                           revol_bal_joint float64\n",
      "                    sec_app_fico_range_low float64\n",
      "                   sec_app_fico_range_high float64\n",
      "                    sec_app_inq_last_6mths float64\n",
      "                          sec_app_mort_acc float64\n",
      "                          sec_app_open_acc float64\n",
      "                        sec_app_revol_util float64\n",
      "                       sec_app_open_act_il float64\n",
      "                     sec_app_num_rev_accts float64\n",
      "          sec_app_chargeoff_within_12_mths float64\n",
      "        sec_app_collections_12_mths_ex_med float64\n",
      "       sec_app_mths_since_last_major_derog float64\n",
      "                             deferral_term float64\n",
      "                           hardship_amount float64\n",
      "                           hardship_length float64\n",
      "                              hardship_dpd float64\n",
      "orig_projected_additional_accrued_interest float64\n",
      "            hardship_payoff_balance_amount float64\n",
      "              hardship_last_payment_amount float64\n",
      "                         settlement_amount float64\n",
      "                     settlement_percentage float64\n",
      "                           settlement_term float64\n",
      "                             inflation_cpi float64\n",
      "                                       gdp float64\n",
      "                         unemployment_rate float64\n",
      "                            fed_funds_rate float64\n",
      "                                       pce float64\n",
      "                     total_consumer_credit float64\n"
     ]
    }
   ],
   "source": [
    "print(cols_types[cols_types['dtype'] == 'float64'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7b30047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_name  dtype\n",
       "0          id  int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_types[cols_types['dtype'] == 'int64']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dff218",
   "metadata": {},
   "source": [
    "Revision of constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23198374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                            192309\n",
      "member_id                                          1\n",
      "loan_amnt                                       1456\n",
      "funded_amnt                                     1457\n",
      "funded_amnt_inv                                 4943\n",
      "term                                               2\n",
      "int_rate                                         500\n",
      "installment                                    42329\n",
      "grade                                              7\n",
      "sub_grade                                         35\n",
      "emp_title                                      92693\n",
      "emp_length                                        12\n",
      "home_ownership                                     6\n",
      "annual_inc                                     15650\n",
      "verification_status                                3\n",
      "issue_d                                          103\n",
      "loan_status                                        9\n",
      "pymnt_plan                                         2\n",
      "url                                           192309\n",
      "desc                                           42236\n",
      "purpose                                           14\n",
      "title                                          25801\n",
      "zip_code                                         892\n",
      "addr_state                                        50\n",
      "dti                                             5070\n",
      "delinq_2yrs                                       22\n",
      "earliest_cr_line                                 665\n",
      "fico_range_low                                    38\n",
      "fico_range_high                                   38\n",
      "inq_last_6mths                                     9\n",
      "mths_since_last_delinq                           131\n",
      "mths_since_last_record                           122\n",
      "open_acc                                          64\n",
      "pub_rec                                           15\n",
      "revol_bal                                      45692\n",
      "revol_util                                      1178\n",
      "total_acc                                        113\n",
      "initial_list_status                                2\n",
      "out_prncp                                      35706\n",
      "out_prncp_inv                                  36213\n",
      "total_pymnt                                   183549\n",
      "total_pymnt_inv                               170864\n",
      "total_rec_prncp                                61547\n",
      "total_rec_int                                 147972\n",
      "total_rec_late_fee                              4558\n",
      "recoveries                                     18125\n",
      "collection_recovery_fee                        16373\n",
      "last_pymnt_d                                     106\n",
      "last_pymnt_amnt                               119856\n",
      "next_pymnt_d                                      69\n",
      "last_credit_pull_d                               107\n",
      "last_fico_range_high                              72\n",
      "last_fico_range_low                               71\n",
      "collections_12_mths_ex_med                         6\n",
      "mths_since_last_major_derog                      143\n",
      "policy_code                                        1\n",
      "application_type                                   2\n",
      "annual_inc_joint                                1632\n",
      "dti_joint                                       2662\n",
      "verification_status_joint                          4\n",
      "acc_now_delinq                                     5\n",
      "tot_coll_amt                                    4350\n",
      "tot_cur_bal                                   112510\n",
      "open_acc_6m                                       15\n",
      "open_act_il                                       42\n",
      "open_il_12m                                       11\n",
      "open_il_24m                                       20\n",
      "mths_since_rcnt_il                               253\n",
      "total_bal_il                                   43950\n",
      "il_util                                          190\n",
      "open_rv_12m                                       22\n",
      "open_rv_24m                                       36\n",
      "max_bal_bc                                     16973\n",
      "all_util                                         150\n",
      "total_rev_hi_lim                                6580\n",
      "inq_fi                                            21\n",
      "total_cu_tl                                       42\n",
      "inq_last_12m                                      35\n",
      "acc_open_past_24mths                              42\n",
      "avg_cur_bal                                    41008\n",
      "bc_open_to_buy                                 36765\n",
      "bc_util                                         1182\n",
      "chargeoff_within_12_mths                           7\n",
      "delinq_amnt                                      332\n",
      "mo_sin_old_il_acct                               442\n",
      "mo_sin_old_rev_tl_op                             663\n",
      "mo_sin_rcnt_rev_tl_op                            216\n",
      "mo_sin_rcnt_tl                                   145\n",
      "mort_acc                                          27\n",
      "mths_since_recent_bc                             369\n",
      "mths_since_recent_bc_dlq                         132\n",
      "mths_since_recent_inq                             27\n",
      "mths_since_recent_revol_delinq                   140\n",
      "num_accts_ever_120_pd                             31\n",
      "num_actv_bc_tl                                    29\n",
      "num_actv_rev_tl                                   43\n",
      "num_bc_sats                                       43\n",
      "num_bc_tl                                         54\n",
      "num_il_tl                                         86\n",
      "num_op_rev_tl                                     58\n",
      "num_rev_accts                                     87\n",
      "num_rev_tl_bal_gt_0                               39\n",
      "num_sats                                          64\n",
      "num_tl_120dpd_2m                                   5\n",
      "num_tl_30dpd                                       6\n",
      "num_tl_90g_dpd_24m                                22\n",
      "num_tl_op_past_12m                                24\n",
      "pct_tl_nvr_dlq                                   477\n",
      "percent_bc_gt_75                                 172\n",
      "pub_rec_bankruptcies                               9\n",
      "tax_liens                                         15\n",
      "tot_hi_cred_lim                               109217\n",
      "total_bal_ex_mort                              82607\n",
      "total_bc_limit                                  5175\n",
      "total_il_high_credit_limit                     69760\n",
      "revol_bal_joint                                 4996\n",
      "sec_app_fico_range_low                            62\n",
      "sec_app_fico_range_high                           62\n",
      "sec_app_earliest_cr_line                         477\n",
      "sec_app_inq_last_6mths                             8\n",
      "sec_app_mort_acc                                  14\n",
      "sec_app_open_acc                                  51\n",
      "sec_app_revol_util                              1013\n",
      "sec_app_open_act_il                               31\n",
      "sec_app_num_rev_accts                             64\n",
      "sec_app_chargeoff_within_12_mths                  12\n",
      "sec_app_collections_12_mths_ex_med                 8\n",
      "sec_app_mths_since_last_major_derog              100\n",
      "hardship_flag                                      2\n",
      "hardship_type                                      2\n",
      "hardship_reason                                   10\n",
      "hardship_status                                    4\n",
      "deferral_term                                      2\n",
      "hardship_amount                                  590\n",
      "hardship_start_date                               27\n",
      "hardship_end_date                                 26\n",
      "payment_plan_start_date                           27\n",
      "hardship_length                                    2\n",
      "hardship_dpd                                      33\n",
      "hardship_loan_status                               5\n",
      "orig_projected_additional_accrued_interest       453\n",
      "hardship_payoff_balance_amount                   597\n",
      "hardship_last_payment_amount                     590\n",
      "disbursement_method                                2\n",
      "debt_settlement_flag                               2\n",
      "debt_settlement_flag_date                         78\n",
      "settlement_status                                  4\n",
      "settlement_date                                   86\n",
      "settlement_amount                               2291\n",
      "settlement_percentage                            449\n",
      "settlement_term                                   29\n",
      "inflation_cpi                                    101\n",
      "gdp                                               35\n",
      "unemployment_rate                                 48\n",
      "fed_funds_rate                                    42\n",
      "pce                                              103\n",
      "total_consumer_credit                            103\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Unique values per column\n",
    "uniq = df.nunique(dropna=False)\n",
    "\n",
    "# Show results\n",
    "print(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25dca59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member_id      1\n",
       "policy_code    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq = df.nunique(dropna=False)\n",
    "uniq[uniq == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "047a4a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member_id\n",
      "None    192309\n",
      "Name: count, dtype: int64\n",
      "policy_code\n",
      "1.0    192309\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols = ['member_id', 'policy_code']\n",
    "\n",
    "for col in cols:\n",
    "    print(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ab5f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['member_id', 'policy_code','id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b604db5",
   "metadata": {},
   "source": [
    "Revision of Duplicated Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea1525a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df9bbd",
   "metadata": {},
   "source": [
    "Missing values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7e7e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp_title                                      6.883193\n",
      "emp_length                                     5.486483\n",
      "desc                                          77.997390\n",
      "title                                          0.595396\n",
      "dti                                            0.039520\n",
      "mths_since_last_delinq                        54.635508\n",
      "mths_since_last_record                        87.276207\n",
      "revol_util                                     0.066040\n",
      "last_pymnt_d                                   0.106079\n",
      "next_pymnt_d                                  75.548206\n",
      "last_credit_pull_d                             0.005200\n",
      "mths_since_last_major_derog                   80.243254\n",
      "annual_inc_joint                              96.915381\n",
      "dti_joint                                     96.915381\n",
      "verification_status_joint                     97.036020\n",
      "tot_coll_amt                                  23.141403\n",
      "tot_cur_bal                                   23.141403\n",
      "open_acc_6m                                   63.193610\n",
      "open_act_il                                   63.193610\n",
      "open_il_12m                                   63.193610\n",
      "open_il_24m                                   63.193610\n",
      "mths_since_rcnt_il                            64.315243\n",
      "total_bal_il                                  63.193610\n",
      "il_util                                       68.555814\n",
      "open_rv_12m                                   63.193610\n",
      "open_rv_24m                                   63.193610\n",
      "max_bal_bc                                    63.193610\n",
      "all_util                                      63.198810\n",
      "total_rev_hi_lim                              23.141403\n",
      "inq_fi                                        63.193610\n",
      "total_cu_tl                                   63.193610\n",
      "inq_last_12m                                  63.193610\n",
      "acc_open_past_24mths                          18.039717\n",
      "avg_cur_bal                                   23.146083\n",
      "bc_open_to_buy                                18.950751\n",
      "bc_util                                       18.993911\n",
      "mo_sin_old_il_acct                            25.589546\n",
      "mo_sin_old_rev_tl_op                          23.141403\n",
      "mo_sin_rcnt_rev_tl_op                         23.141403\n",
      "mo_sin_rcnt_tl                                23.141403\n",
      "mort_acc                                      18.039717\n",
      "mths_since_recent_bc                          18.885752\n",
      "mths_since_recent_bc_dlq                      81.567165\n",
      "mths_since_recent_inq                         27.157855\n",
      "mths_since_recent_revol_delinq                73.015303\n",
      "num_accts_ever_120_pd                         23.141403\n",
      "num_actv_bc_tl                                23.141403\n",
      "num_actv_rev_tl                               23.141403\n",
      "num_bc_sats                                   20.619940\n",
      "num_bc_tl                                     23.141403\n",
      "num_il_tl                                     23.141403\n",
      "num_op_rev_tl                                 23.141403\n",
      "num_rev_accts                                 23.141923\n",
      "num_rev_tl_bal_gt_0                           23.141403\n",
      "num_sats                                      20.619940\n",
      "num_tl_120dpd_2m                              25.534426\n",
      "num_tl_30dpd                                  23.141403\n",
      "num_tl_90g_dpd_24m                            23.141403\n",
      "num_tl_op_past_12m                            23.141403\n",
      "pct_tl_nvr_dlq                                23.153363\n",
      "percent_bc_gt_75                              18.968431\n",
      "tot_hi_cred_lim                               23.141403\n",
      "total_bal_ex_mort                             18.039717\n",
      "total_bc_limit                                18.039717\n",
      "total_il_high_credit_limit                    23.141403\n",
      "revol_bal_joint                               97.286658\n",
      "sec_app_fico_range_low                        97.286658\n",
      "sec_app_fico_range_high                       97.286658\n",
      "sec_app_earliest_cr_line                      97.286658\n",
      "sec_app_inq_last_6mths                        97.286658\n",
      "sec_app_mort_acc                              97.286658\n",
      "sec_app_open_acc                              97.286658\n",
      "sec_app_revol_util                            97.327738\n",
      "sec_app_open_act_il                           97.286658\n",
      "sec_app_num_rev_accts                         97.286658\n",
      "sec_app_chargeoff_within_12_mths              97.286658\n",
      "sec_app_collections_12_mths_ex_med            97.286658\n",
      "sec_app_mths_since_last_major_derog           99.091566\n",
      "hardship_type                                 99.690082\n",
      "hardship_reason                               99.690082\n",
      "hardship_status                               99.690082\n",
      "deferral_term                                 99.690082\n",
      "hardship_amount                               99.690082\n",
      "hardship_start_date                           99.690082\n",
      "hardship_end_date                             99.690082\n",
      "payment_plan_start_date                       99.690082\n",
      "hardship_length                               99.690082\n",
      "hardship_dpd                                  99.690082\n",
      "hardship_loan_status                          99.690082\n",
      "orig_projected_additional_accrued_interest    99.762882\n",
      "hardship_payoff_balance_amount                99.690082\n",
      "hardship_last_payment_amount                  99.690082\n",
      "debt_settlement_flag_date                     98.700009\n",
      "settlement_status                             98.700009\n",
      "settlement_date                               98.700009\n",
      "settlement_amount                             98.700009\n",
      "settlement_percentage                         98.700009\n",
      "settlement_term                               98.700009\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing = df.isna().mean()*100\n",
    "print(missing[missing>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18e3d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt                                      0.000000\n",
      "funded_amnt                                    0.000000\n",
      "funded_amnt_inv                                0.000000\n",
      "term                                           0.000000\n",
      "int_rate                                       0.000000\n",
      "installment                                    0.000000\n",
      "grade                                          0.000000\n",
      "sub_grade                                      0.000000\n",
      "emp_title                                      6.883193\n",
      "emp_length                                     5.486483\n",
      "home_ownership                                 0.000000\n",
      "annual_inc                                     0.000000\n",
      "verification_status                            0.000000\n",
      "issue_d                                        0.000000\n",
      "loan_status                                    0.000000\n",
      "pymnt_plan                                     0.000000\n",
      "url                                            0.000000\n",
      "desc                                          77.997390\n",
      "purpose                                        0.000000\n",
      "title                                          0.595396\n",
      "zip_code                                       0.000000\n",
      "addr_state                                     0.000000\n",
      "dti                                            0.039520\n",
      "delinq_2yrs                                    0.000000\n",
      "earliest_cr_line                               0.000000\n",
      "fico_range_low                                 0.000000\n",
      "fico_range_high                                0.000000\n",
      "inq_last_6mths                                 0.000000\n",
      "mths_since_last_delinq                        54.635508\n",
      "mths_since_last_record                        87.276207\n",
      "open_acc                                       0.000000\n",
      "pub_rec                                        0.000000\n",
      "revol_bal                                      0.000000\n",
      "revol_util                                     0.066040\n",
      "total_acc                                      0.000000\n",
      "initial_list_status                            0.000000\n",
      "out_prncp                                      0.000000\n",
      "out_prncp_inv                                  0.000000\n",
      "total_pymnt                                    0.000000\n",
      "total_pymnt_inv                                0.000000\n",
      "total_rec_prncp                                0.000000\n",
      "total_rec_int                                  0.000000\n",
      "total_rec_late_fee                             0.000000\n",
      "recoveries                                     0.000000\n",
      "collection_recovery_fee                        0.000000\n",
      "last_pymnt_d                                   0.106079\n",
      "last_pymnt_amnt                                0.000000\n",
      "next_pymnt_d                                  75.548206\n",
      "last_credit_pull_d                             0.005200\n",
      "last_fico_range_high                           0.000000\n",
      "last_fico_range_low                            0.000000\n",
      "collections_12_mths_ex_med                     0.000000\n",
      "mths_since_last_major_derog                   80.243254\n",
      "application_type                               0.000000\n",
      "annual_inc_joint                              96.915381\n",
      "dti_joint                                     96.915381\n",
      "verification_status_joint                     97.036020\n",
      "acc_now_delinq                                 0.000000\n",
      "tot_coll_amt                                  23.141403\n",
      "tot_cur_bal                                   23.141403\n",
      "open_acc_6m                                   63.193610\n",
      "open_act_il                                   63.193610\n",
      "open_il_12m                                   63.193610\n",
      "open_il_24m                                   63.193610\n",
      "mths_since_rcnt_il                            64.315243\n",
      "total_bal_il                                  63.193610\n",
      "il_util                                       68.555814\n",
      "open_rv_12m                                   63.193610\n",
      "open_rv_24m                                   63.193610\n",
      "max_bal_bc                                    63.193610\n",
      "all_util                                      63.198810\n",
      "total_rev_hi_lim                              23.141403\n",
      "inq_fi                                        63.193610\n",
      "total_cu_tl                                   63.193610\n",
      "inq_last_12m                                  63.193610\n",
      "acc_open_past_24mths                          18.039717\n",
      "avg_cur_bal                                   23.146083\n",
      "bc_open_to_buy                                18.950751\n",
      "bc_util                                       18.993911\n",
      "chargeoff_within_12_mths                       0.000000\n",
      "delinq_amnt                                    0.000000\n",
      "mo_sin_old_il_acct                            25.589546\n",
      "mo_sin_old_rev_tl_op                          23.141403\n",
      "mo_sin_rcnt_rev_tl_op                         23.141403\n",
      "mo_sin_rcnt_tl                                23.141403\n",
      "mort_acc                                      18.039717\n",
      "mths_since_recent_bc                          18.885752\n",
      "mths_since_recent_bc_dlq                      81.567165\n",
      "mths_since_recent_inq                         27.157855\n",
      "mths_since_recent_revol_delinq                73.015303\n",
      "num_accts_ever_120_pd                         23.141403\n",
      "num_actv_bc_tl                                23.141403\n",
      "num_actv_rev_tl                               23.141403\n",
      "num_bc_sats                                   20.619940\n",
      "num_bc_tl                                     23.141403\n",
      "num_il_tl                                     23.141403\n",
      "num_op_rev_tl                                 23.141403\n",
      "num_rev_accts                                 23.141923\n",
      "num_rev_tl_bal_gt_0                           23.141403\n",
      "num_sats                                      20.619940\n",
      "num_tl_120dpd_2m                              25.534426\n",
      "num_tl_30dpd                                  23.141403\n",
      "num_tl_90g_dpd_24m                            23.141403\n",
      "num_tl_op_past_12m                            23.141403\n",
      "pct_tl_nvr_dlq                                23.153363\n",
      "percent_bc_gt_75                              18.968431\n",
      "pub_rec_bankruptcies                           0.000000\n",
      "tax_liens                                      0.000000\n",
      "tot_hi_cred_lim                               23.141403\n",
      "total_bal_ex_mort                             18.039717\n",
      "total_bc_limit                                18.039717\n",
      "total_il_high_credit_limit                    23.141403\n",
      "revol_bal_joint                               97.286658\n",
      "sec_app_fico_range_low                        97.286658\n",
      "sec_app_fico_range_high                       97.286658\n",
      "sec_app_earliest_cr_line                      97.286658\n",
      "sec_app_inq_last_6mths                        97.286658\n",
      "sec_app_mort_acc                              97.286658\n",
      "sec_app_open_acc                              97.286658\n",
      "sec_app_revol_util                            97.327738\n",
      "sec_app_open_act_il                           97.286658\n",
      "sec_app_num_rev_accts                         97.286658\n",
      "sec_app_chargeoff_within_12_mths              97.286658\n",
      "sec_app_collections_12_mths_ex_med            97.286658\n",
      "sec_app_mths_since_last_major_derog           99.091566\n",
      "hardship_flag                                  0.000000\n",
      "hardship_type                                 99.690082\n",
      "hardship_reason                               99.690082\n",
      "hardship_status                               99.690082\n",
      "deferral_term                                 99.690082\n",
      "hardship_amount                               99.690082\n",
      "hardship_start_date                           99.690082\n",
      "hardship_end_date                             99.690082\n",
      "payment_plan_start_date                       99.690082\n",
      "hardship_length                               99.690082\n",
      "hardship_dpd                                  99.690082\n",
      "hardship_loan_status                          99.690082\n",
      "orig_projected_additional_accrued_interest    99.762882\n",
      "hardship_payoff_balance_amount                99.690082\n",
      "hardship_last_payment_amount                  99.690082\n",
      "disbursement_method                            0.000000\n",
      "debt_settlement_flag                           0.000000\n",
      "debt_settlement_flag_date                     98.700009\n",
      "settlement_status                             98.700009\n",
      "settlement_date                               98.700009\n",
      "settlement_amount                             98.700009\n",
      "settlement_percentage                         98.700009\n",
      "settlement_term                               98.700009\n",
      "inflation_cpi                                  0.000000\n",
      "gdp                                            0.000000\n",
      "unemployment_rate                              0.000000\n",
      "fed_funds_rate                                 0.000000\n",
      "pce                                            0.000000\n",
      "total_consumer_credit                          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_threshold = 0\n",
    "\n",
    "high_missing_cols = missing[missing >= missing_threshold]\n",
    "print(high_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d45cade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column categorization complete\n"
     ]
    }
   ],
   "source": [
    "# Column categorization\n",
    "def classify_column(col):\n",
    "    if col.startswith((\"hardship\", \"settlement\", \"deferral\")):\n",
    "        return \"post_loan\"\n",
    "    if col.startswith(\"sec_app\") or col.endswith(\"_joint\"):\n",
    "        return \"second_applicant\"\n",
    "    if col.startswith(\"mths_since\"):\n",
    "        return \"structural_missing\"\n",
    "    return \"other\"\n",
    "\n",
    "categories = [\"post_loan\", \"second_applicant\", \"structural_missing\"]\n",
    "analysis_dict = {cat: [col for col in df.columns if classify_column(col) == cat] for cat in categories}\n",
    "\n",
    "print(\"Column categorization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc556720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 34 columns\n"
     ]
    }
   ],
   "source": [
    "# Remove post-loan and second-applicant columns\n",
    "post_loan_cols = analysis_dict[\"post_loan\"]\n",
    "second_applicant = analysis_dict[\"second_applicant\"]\n",
    "\n",
    "cols_to_drop = post_loan_cols + second_applicant\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(f\"Dropped {len(cols_to_drop)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9c6aa",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e075dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_status\n",
      "Fully Paid                                             119172\n",
      "Current                                                 44568\n",
      "Charged Off                                             26114\n",
      "Late (31-120 days)                                       1141\n",
      "Does not meet the credit policy. Status:Fully Paid        442\n",
      "In Grace Period                                           436\n",
      "Late (16-30 days)                                         249\n",
      "Does not meet the credit policy. Status:Charged Off       186\n",
      "Default                                                     1\n",
      "Name: count, dtype: int64\n",
      "Filtered to 192308 rows\n"
     ]
    }
   ],
   "source": [
    "# Filter frequent classes only\n",
    "print(df['loan_status'].value_counts())\n",
    "\n",
    "frequent_classes = df['loan_status'].value_counts()[df['loan_status'].value_counts() > 5].index\n",
    "df = df[df['loan_status'].isin(frequent_classes)]\n",
    "\n",
    "print(f\"Filtered to {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc8740f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (153846, 119), Test: (38462, 119)\n"
     ]
    }
   ],
   "source": [
    "# Create X, y split\n",
    "X = df.drop(columns=['loan_status'])\n",
    "y = df['loan_status']\n",
    "\n",
    "# Split 80% train / 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5e415",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8996ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed\n"
     ]
    }
   ],
   "source": [
    "# Convert term to numeric\n",
    "X_train['term'] = X_train['term'].astype(str).str.replace(' months','').astype(int)\n",
    "X_test['term'] = X_test['term'].astype(str).str.replace(' months','').astype(int)\n",
    "\n",
    "# Remove free text columns\n",
    "cols_to_drop = ['emp_title', 'url', 'title', 'desc']\n",
    "X_train = X_train.drop(columns=[c for c in cols_to_drop if c in X_train.columns])\n",
    "X_test = X_test.drop(columns=[c for c in cols_to_drop if c in X_test.columns])\n",
    "\n",
    "# Handle zip_code\n",
    "X_train['zip_code'] = X_train['zip_code'].astype(str).str[:3]\n",
    "X_test['zip_code'] = X_test['zip_code'].astype(str).str[:3]\n",
    "\n",
    "ord_enc_zip = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train[['zip_code']] = ord_enc_zip.fit_transform(X_train[['zip_code']])\n",
    "X_test[['zip_code']] = ord_enc_zip.transform(X_test[['zip_code']])\n",
    "\n",
    "print(\"Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93f77c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (153846, 170)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical variables\n",
    "categorical_cols = ['grade', 'sub_grade', 'home_ownership', \n",
    "                    'verification_status', 'purpose', \n",
    "                    'initial_list_status', 'application_type']\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "X_train_encoded = pd.DataFrame(ohe.transform(X_train[categorical_cols]), \n",
    "                               columns=ohe.get_feature_names_out(categorical_cols), \n",
    "                               index=X_train.index)\n",
    "X_test_encoded = pd.DataFrame(ohe.transform(X_test[categorical_cols]), \n",
    "                              columns=ohe.get_feature_names_out(categorical_cols), \n",
    "                              index=X_test.index)\n",
    "\n",
    "X_train = X_train.drop(columns=categorical_cols).join(X_train_encoded)\n",
    "X_test = X_test.drop(columns=categorical_cols).join(X_test_encoded)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e77ba23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary encoding completed\n"
     ]
    }
   ],
   "source": [
    "# Encode binary columns\n",
    "binary_cols = ['pymnt_plan', 'debt_settlement_flag']\n",
    "for col in binary_cols:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].map({'y': 1, 'n': 0})\n",
    "        X_test[col] = X_test[col].map({'y': 1, 'n': 0})\n",
    "\n",
    "print(\"Binary encoding completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4db479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State encoding completed\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode addr_state\n",
    "if 'addr_state' in X_train.columns:\n",
    "    ohe_state = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "    ohe_state.fit(X_train[['addr_state']])\n",
    "\n",
    "    X_train_state = pd.DataFrame(ohe_state.transform(X_train[['addr_state']]),\n",
    "                                 columns=ohe_state.get_feature_names_out(['addr_state']),\n",
    "                                 index=X_train.index)\n",
    "    X_test_state = pd.DataFrame(ohe_state.transform(X_test[['addr_state']]),\n",
    "                                columns=ohe_state.get_feature_names_out(['addr_state']),\n",
    "                                index=X_test.index)\n",
    "\n",
    "    X_train = X_train.drop(columns=['addr_state']).join(X_train_state)\n",
    "    X_test = X_test.drop(columns=['addr_state']).join(X_test_state)\n",
    "\n",
    "print(\"State encoding completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16ba03",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fc361c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "loan_status\n",
      "0    131340\n",
      "1     22506\n",
      "Name: count, dtype: int64\n",
      "loan_status\n",
      "0    32836\n",
      "1     5626\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encode target to binary\n",
    "default_labels = {\n",
    "    'Charged Off': 1,\n",
    "    'Does not meet the credit policy. Status:Charged Off': 1,\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 1,\n",
    "    'Late (16-30 days)': 1,\n",
    "    'Late (31-120 days)': 1,\n",
    "    'Fully Paid': 0,\n",
    "    'Current': 0,\n",
    "    'In Grace Period': 0\n",
    "}\n",
    "\n",
    "y_train = y_train.map(default_labels)\n",
    "y_test = y_test.map(default_labels)\n",
    "\n",
    "print(\"Target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33c9e1",
   "metadata": {},
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "977872b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '4 years'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_42384\\1236526298.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Apply IsolationForest\u001b[39;00m\n\u001b[32m      2\u001b[39m df_train_clean = X_train.copy()\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m iso = IsolationForest(random_state=\u001b[32m123\u001b[39m, contamination=\u001b[33m'auto'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_train_clean[\u001b[33m'outlier_flag'\u001b[39m] = iso.fit_predict(df_train_clean)\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Remove outliers\u001b[39;00m\n\u001b[32m      8\u001b[39m df_train_clean = df_train_clean[df_train_clean[\u001b[33m'outlier_flag'\u001b[39m] == \u001b[32m1\u001b[39m]\n",
      "\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m   1125\u001b[39m                     UserWarning,\n\u001b[32m   1126\u001b[39m                 )\n\u001b[32m   1127\u001b[39m \n\u001b[32m   1128\u001b[39m         \u001b[38;5;66;03m# override for transductive outlier detectors like LocalOulierFactor\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **kwargs).predict(X)\n",
      "\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1332\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m                 )\n\u001b[32m   1335\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    317\u001b[39m         -------\n\u001b[32m    318\u001b[39m         self : object\n\u001b[32m    319\u001b[39m             Fitted estimator.\n\u001b[32m    320\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m         X = validate_data(\n\u001b[32m    322\u001b[39m             self, X, accept_sparse=[\u001b[33m\"csc\"\u001b[39m], dtype=tree_dtype, ensure_all_finite=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    323\u001b[39m         )\n\u001b[32m    324\u001b[39m \n",
      "\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2898\u001b[39m             out = y\n\u001b[32m   2899\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2900\u001b[39m             out = X, y\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2903\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2905\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1019\u001b[39m                         )\n\u001b[32m   1020\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1021\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1022\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1024\u001b[39m                 raise ValueError(\n\u001b[32m   1025\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1026\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    874\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    875\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    876\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    877\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    879\u001b[39m \n\u001b[32m    880\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2167\u001b[39m             )\n\u001b[32m   2168\u001b[39m         values = self._values\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2172\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2173\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2174\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '4 years'"
     ]
    }
   ],
   "source": [
    "# Apply IsolationForest\n",
    "df_train_clean = X_train.copy()\n",
    "\n",
    "iso = IsolationForest(random_state=123, contamination='auto')\n",
    "df_train_clean['outlier_flag'] = iso.fit_predict(df_train_clean)\n",
    "\n",
    "# Remove outliers\n",
    "df_train_clean = df_train_clean[df_train_clean['outlier_flag'] == 1]\n",
    "df_train_clean = df_train_clean.drop(columns=['outlier_flag'])\n",
    "y_train_clean = y_train.loc[df_train_clean.index]\n",
    "\n",
    "print(f\"Rows after removing outliers: {df_train_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25703e",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Random Forest Feature Importance\n",
    "model_rf = RandomForestClassifier(random_state=42, n_jobs=-1).fit(df_train_clean, y_train_clean)\n",
    "importances = model_rf.feature_importances_ / model_rf.feature_importances_.sum() * 100\n",
    "\n",
    "df_rf_imp = pd.DataFrame({\n",
    "    'feature': df_train_clean.columns,\n",
    "    'rf_importance': importances\n",
    "}).sort_values(by='rf_importance', ascending=False)\n",
    "\n",
    "df_rf_imp['rf_importance_acum'] = df_rf_imp['rf_importance'].cumsum()\n",
    "print(df_rf_imp.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa99513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Permutation Importance\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(\n",
    "    df_train_clean, y_train_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ").fit(X_train1, y_train1)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    model_xgb, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1, scoring='accuracy'\n",
    ")\n",
    "\n",
    "df_perm_imp = pd.DataFrame({\n",
    "    'feature': df_train_clean.columns,\n",
    "    'perm_imp': perm.importances_mean * 100\n",
    "}).sort_values('perm_imp', ascending=False)\n",
    "\n",
    "print(df_perm_imp.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44bc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: SHAP Values\n",
    "model_lgbm = lgb.LGBMClassifier(random_state=42, n_jobs=-1).fit(df_train_clean, y_train_clean)\n",
    "\n",
    "explainer = shap.Explainer(model_lgbm, X_val)\n",
    "shap_vals = explainer(X_val).values\n",
    "\n",
    "imp_shap = np.abs(shap_vals).mean(axis=0)\n",
    "imp_shap_pct = imp_shap / imp_shap.sum() * 100\n",
    "\n",
    "df_shap_imp = pd.DataFrame({\n",
    "    \"feature\": X_val.columns,\n",
    "    \"shap_imp\": imp_shap_pct\n",
    "}).sort_values('shap_imp', ascending=False)\n",
    "\n",
    "print(df_shap_imp.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33002668",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install missingno MissForest lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa5eb6",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "El DataFrame df se crea a partir de la tabla basa_datos_pripal de la base de datos credit_scoring.db. El archivo no se incluye en el repositorio debido a su tamaño; la generación de la base de datos a partir del CSV original y la creación de la tabla se explica detalladamente en el notebook data-collection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13c2f7",
   "metadata": {},
   "source": [
    "Descriptive analysis:\n",
    "In this part of the project, we begin exploring the dataset created from the initial information obtained from the LendingClub dataset (Kaggle). The objective of this stage is to describe and understand the structure of the data, the variables and their types, their distributions, skewness, and the presence of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ae9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/workspaces/final_project_creditscoring/Data/credit_scoring.db\")\n",
    "df = pd.read_sql(\"SELECT * FROM main_table\", conn)\n",
    "conn.close()\n",
    "n_rows,n_cols = df.shape\n",
    "\n",
    "print(f'En este df existen {n_rows} filas y {n_cols} columnas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2e70c",
   "metadata": {},
   "source": [
    "With the analysis below, we can understand that the dataset contains a large number of numerical variables, along with several categorical features represented as object types. This initial inspection highlights the need for feature selection and type handling in later stages. Now we are proceeding with a list of each type of column to identify possible issues in data types (such as date columns that are objects or numerical data that is shown as objects, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_types = df.dtypes.reset_index().rename(\n",
    "    columns={'index': 'column_name', 0: 'dtype'}\n",
    ")\n",
    "\n",
    "cols_types['dtype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_types[cols_types['dtype'] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88aac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols_types[cols_types['dtype'] == 'float64'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_types[cols_types['dtype'] == 'int64']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5abd37",
   "metadata": {},
   "source": [
    "Revision of constant columns: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29069c80",
   "metadata": {},
   "source": [
    "The following code initially didn't specify dropna=False, which made it show a few columns as constants. This led us to investigate what was happening and whether we were working with the right dataframe. However, this mistake was enlightening, as it helped us identify possible *data leakage variables, such as: hardship_type, deferral_term, and hardship_length.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec995b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Unique values per column\n",
    "uniq = df.nunique(dropna=False)\n",
    "\n",
    "# Show results\n",
    "print(uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee830d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq = df.nunique(dropna=False)\n",
    "uniq[uniq == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e33bf",
   "metadata": {},
   "source": [
    "After the code revision, the only two variables with constant values are member_id and policy_code. It does not make much sense to have a unique value for member_id if we have almost 200k data entries, so we needed to check the exact values contained in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['member_id', 'policy_code']\n",
    "\n",
    "for col in cols:\n",
    "    print(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a7540",
   "metadata": {},
   "source": [
    "Regarding policy_code, a similar situation occurs. According to the data dictionary, LendingClub has only two types of policies: publicly available (1) and new products not publicly available (2). In this dataset, only publicly available products are present. Therefore, following the same reasoning as above, policy_code is not a relevant column for the analysis.\n",
    "\n",
    "Lastly, as we are dropping member_id and policy_code because they are not predictors, we are doing the same with two other columns as well, which, even though they do not have constant values, can generate noise in the analysis: the url column and the id column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eefc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['member_id', 'policy_code','id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec378be9",
   "metadata": {},
   "source": [
    "Revision of Duplicated Rows: No duplicated rows were identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93e2c9",
   "metadata": {},
   "source": [
    "Revision of Duplicated Columns: Two variables (deferral_term and hardship_length) were found to be exact duplicates, containing identical values across all observations. Both variables are related to post-loan hardship events (we previously identified them as potential data leakers) and will therefore be excluded from the modeling stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf46343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.duplicated().sum()\n",
    "df.T.duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477f0db",
   "metadata": {},
   "source": [
    "Missing values: \n",
    "We identified columns with a high percentage of missing values, so we proceeded to define a missing threshold of 50%, where variables with more than 50% missing values will be considered for exclusion from the modeling stage. However, first we must evaluate them on a case-by-case basis to understand if any of those variables are conceptually important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isna().mean()*100\n",
    "missing[missing>0]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0\n",
    "\n",
    "high_missing_cols = missing[missing >= missing_threshold]\n",
    "print(high_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c822944",
   "metadata": {},
   "outputs": [],
   "source": [
    "(missing > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d3945",
   "metadata": {},
   "source": [
    "Now we have to identify other missing values and audit them to understand how we should treat each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158403ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify other missing values\n",
    "cat_col = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in cat_col: \n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faltantes = df.replace(['None'],np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad296c82",
   "metadata": {},
   "source": [
    "Columns with more than 50% missing values were manually reviewed and classified into post-loan variables, second-applicant features, structurally missing variables, and late-reported behavioral features based on domain knowledge and data documentation. No features were removed at this stage; the analysis documents decisions to be applied during model preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_column(col):\n",
    "    if col.startswith((\"hardship\", \"settlement\", \"deferral\")):\n",
    "        return \"post_loan\"\n",
    "    if col.startswith(\"sec_app\") or col.endswith(\"_joint\"):\n",
    "        return \"second_applicant\"\n",
    "    if col.startswith(\"mths_since\"):\n",
    "        return \"structural_missing\"\n",
    "    return \"other\"\n",
    "\n",
    "categories = [\"post_loan\", \"second_applicant\", \"structural_missing\"]\n",
    "analysis_dict = {cat: [col for col in df.columns if classify_column(col) == cat] for cat in categories}\n",
    "\n",
    "print(\"🔍 --- STARTING COLUMN CATEGORIZATION ANALYSIS --- 🔍\\n\")\n",
    "\n",
    "for category, cols in analysis_dict.items():\n",
    "    print(f\"📁 CATEGORY: {category.upper()}\")\n",
    "    if not cols:\n",
    "        print(\"   ❌ No columns found in this category.\\n\")\n",
    "    else:\n",
    "        print(f\"   ✅ Found {len(cols)} columns.\")\n",
    "        missing_stats = df[cols].isnull().mean() * 100\n",
    "        print(missing_stats.sort_values(ascending=False).to_string())\n",
    "        print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "print(\"🚀 --- ANALYSIS COMPLETE --- 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column checker: to be able to quickly check the characteristics of a column and its type\n",
    "\n",
    "target_col = 'loan_status'  \n",
    "\n",
    "col_type = df[target_col].dtype\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Count': df[target_col].value_counts(dropna=False),\n",
    "    'Percentage (%)': df[target_col].value_counts(dropna=False, normalize=True) * 100\n",
    "})\n",
    "\n",
    "print(f\"Content Analysis for: {target_col.upper()}\")\n",
    "print(summary)\n",
    "print('---'*30)\n",
    "print(f\"Data Type: {col_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e30f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classified_cols = (analysis_dict['post_loan'] + \n",
    "                      analysis_dict['second_applicant'] + \n",
    "                      analysis_dict['structural_missing'])\n",
    "\n",
    "other_cols = [col for col in df.columns if col not in all_classified_cols]\n",
    "\n",
    "print(\"🔍 --- AUDITING 'OTHER' COLUMNS WITH HIGH MISSING RATIO (>40%) ---\")\n",
    "high_missing_other = df[other_cols].isnull().mean()\n",
    "high_missing_other = high_missing_other[high_missing_other > 0.4].sort_values(ascending=False)\n",
    "\n",
    "if high_missing_other.empty:\n",
    "    print(\"✅ No additional critical missing values found outside defined categories.\")\n",
    "else:\n",
    "    print(\"⚠️ Attention: The following columns also have a high missing ratio:\")\n",
    "    print(high_missing_other.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434a688",
   "metadata": {},
   "source": [
    "1. *Post-loan variables:*\n",
    "\n",
    "These features contain information generated after loan origination, such as hardship or settlement events. Their high missingness reflects the fact that most loans do not enter these processes. Because these variables include future information relative to the credit decision, they were identified as potential sources of data leakage.\n",
    "\n",
    "Planned decision: Exclude.\n",
    "\n",
    "2. *Second-applicant variables:*\n",
    "\n",
    "These variables describe characteristics of a co-borrower in joint loan applications. The high proportion of missing values reflects that most loans involve a single applicant, meaning missing values indicate the absence of a second applicant rather than missing information.\n",
    "\n",
    "Rather than modeling the full co-borrower profile, the presence of a second applicant is captured through a binary indicator. This approach preserves potentially relevant information while avoiding additional complexity and extensive imputation.\n",
    "\n",
    "Planned decision: Create a binary flag indicating whether a loan includes a second applicant, and exclude detailed second-applicant features during model preparation.\n",
    "\n",
    "3. *Structurally missing variables:*\n",
    "\n",
    "These features represent the time since the last occurrence of negative credit events. Missing values indicate that the event has never occurred, making the missingness itself informative.\n",
    "\n",
    "Planned decision: Retain for modeling and apply a dedicated imputation strategy at a later stage.\n",
    "\n",
    "4. *Late-reported features:*\n",
    "\n",
    "These variables were introduced into the dataset at later periods and are unavailable for older loans. Missingness is driven by historical reporting limitations rather than borrower behavior.\n",
    "\n",
    "Planned decision: Evaluate after defining the temporal train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41fd0a",
   "metadata": {},
   "source": [
    "DATA CLEANING & PREPROCESSING STRATEGY\n",
    "\n",
    "Now that we have a clearer understanding of the data, we can proceed with data cleaning and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bea0a6",
   "metadata": {},
   "source": [
    "1. DF Backup: Create a full copy of the raw dataset to ensure data integrity and allow for easy rollbacks during the experimentation phase.\n",
    "\n",
    "2. Target Definition & Filtering: Refine the loan_status variable. We exclude ongoing loans and focus only on definitive outcomes.\n",
    "\n",
    "Default (1): Charged off, default, or late (30–120 days).\n",
    "Charged Off\n",
    "Late (31-120 days)\n",
    "Default\n",
    "Does not meet the credit policy. Status:Charged Off\n",
    "\n",
    "Non-Default (0): Fully paid.\n",
    "Fully Paid\n",
    "\n",
    "3. Leakage Removal: Drop all Post-loan variables. These features contain information only available after the credit decision has been made, which would lead to Data Leakage.\n",
    "\n",
    "4. Structural Simplification (Joint Apps): Consolidate +16 second-applicant features into a single Binary Flag (is_joint_application). This reduces dimensionality while preserving the fact that a co-borrower exists.\n",
    "\n",
    "5. Zero Ratio & Variance Analysis: Identify features with excessive sparsity. We decide whether to drop columns with near-zero variance or binarize features where the simple presence of an event (0 vs >0) is more predictive than its frequency.\n",
    "\n",
    "6. Missingness Audit (Missingno): Visualize the remaining missing values to determine the mechanism of missingness (Random vs. Structural). This dictates the final decision: drop the column (if >50% NaN) or keep it for Imputation after the Train-Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect unique values and their prevalence\n",
    "def inspect_categories(dataframe, column_list):\n",
    "    \"\"\"\n",
    "    Prints frequency and percentage distribution for categorical features.\n",
    "    \"\"\"\n",
    "    for col in column_list:\n",
    "        print(f\"\\n--- Feature: {col.upper()} ---\")\n",
    "        \n",
    "        counts = dataframe[col].value_counts(dropna=False)\n",
    "        percentages = dataframe[col].value_counts(dropna=False, normalize=True) * 100\n",
    "        \n",
    "        summary = pd.DataFrame({\n",
    "            'Count': counts,\n",
    "            'Percentage (%)': percentages.round(2)\n",
    "        })\n",
    "        \n",
    "        print(summary)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "target_cols = analysis_dict.get('other', [])\n",
    "inspect_categories(df, target_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e51a1f",
   "metadata": {},
   "source": [
    "Now we proceed with a zero ratio analysis, that has the objective of helping us decide which variables doesn't have enough information to support the model and which ones are corrupt with 0s that should be NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086315d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero Ratio Analysis\n",
    "zero_ratio = (df == 0).mean() * 100\n",
    "zero_ratio_all = zero_ratio[zero_ratio > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"ALL COLUMNS WITH ZEROS\")\n",
    "print(zero_ratio_all.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf39b1",
   "metadata": {},
   "source": [
    "Having identified the columns with higher amounts of 0s, we decided to audit them one by one and review their unique values to understand whether the 0s are informative or if they represent null/NaN values. This audit is half based on the code shown below and half on a manual review of the dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Zero Ratio again to get the target columns\n",
    "zero_ratio = (df == 0).mean() * 100\n",
    "# Define a threshold (e.g., columns with more than 50% zeros)\n",
    "high_zero_threshold = 30.0\n",
    "high_zero_cols = zero_ratio[zero_ratio > high_zero_threshold].sort_values(ascending=False).index.tolist()\n",
    "\n",
    "def audit_high_zero_columns(dataframe, column_list):\n",
    "    \"\"\"\n",
    "    Audits columns with high zero ratios to see value distribution \n",
    "    and help decide between dropping, keeping, or binarizing.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 --- AUDITING {len(column_list)} COLUMNS WITH > {high_zero_threshold}% ZEROS --- \\n\")\n",
    "    \n",
    "    for col in column_list:\n",
    "        print(f\"📊 Feature: {col.upper()}\")\n",
    "        print(f\"Zero Ratio: {zero_ratio[col]:.2f}%\")\n",
    "        \n",
    "        # Count unique values excluding zero\n",
    "        non_zero_values = dataframe[dataframe[col] != 0][col]\n",
    "        unique_counts = non_zero_values.nunique()\n",
    "        \n",
    "        print(f\"Unique values (excluding zero): {unique_counts}\")\n",
    "        \n",
    "        if unique_counts < 15:\n",
    "            # If few unique values, show frequency\n",
    "            print(\"Distribution (Top Values):\")\n",
    "            print(dataframe[col].value_counts().head(10))\n",
    "        else:\n",
    "            # If many unique values, show basic stats for non-zero data\n",
    "            print(\"Non-zero stats:\")\n",
    "            print(non_zero_values.describe()[['mean', 'min', 'max']])\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Execute the audit\n",
    "audit_high_zero_columns(df, high_zero_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c50062",
   "metadata": {},
   "source": [
    "The function below is intended to serve as a filter to help us quickly verify whether a column was correctly classified in the categories defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_category(column_name, mapping_dict):\n",
    "    \"\"\"\n",
    "    Checks which category a specific column belongs to based on the analysis_dict.\n",
    "    \"\"\"\n",
    "    for category, columns in mapping_dict.items():\n",
    "        if column_name in columns:\n",
    "            return category\n",
    "    return \"other (or not found)\"\n",
    "\n",
    "test_col = 'loan_status' # You can change this name to any column\n",
    "result = get_column_category(test_col, analysis_dict)\n",
    "print(f\"Verification: The column '{test_col}' is categorized as: {result.upper()}\")\n",
    "\n",
    "# 3. Batch verification (Optional)\n",
    "# List of columns you want to verify right now\n",
    "verify_list = ['sec_app_fico_range_low', 'mths_since_last_delinq', 'loan_amnt', 'settlement_term']\n",
    "\n",
    "print(\"BATCH VERIFICATION:\")\n",
    "for col in verify_list:\n",
    "    cat = get_column_category(col, analysis_dict)\n",
    "    print(f\"- {col:30} -> Category: {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78464e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    vc = df[col].value_counts(dropna=False, normalize=True)\n",
    "    if vc.iloc[0] > 0.99:   # más del 99% el mismo valor\n",
    "        low_variance_cols.append(col)\n",
    "\n",
    "low_variance_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df.describe().T\n",
    "desc.sort_values(by='max', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d528ab",
   "metadata": {},
   "source": [
    "The analysis of the maximum values reveals the presence of extreme values in some financial variables, which suggests the need to apply transformations or outlier treatment techniques in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce684fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.assign(\n",
    "    mean_median_ratio = desc['mean'] / desc['50%']\n",
    ").sort_values('mean_median_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708f316",
   "metadata": {},
   "source": [
    "Several numerical variables present highly skewed distributions, with median values equal to zero and a small proportion of non-zero observations. This pattern is expected for count-based credit history variables. However, some highly skewed variables correspond to post-loan information and will therefore be excluded from the modeling process to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a07b3",
   "metadata": {},
   "source": [
    "EDA (cerrando)\n",
    "1. Tratamiento columnas con alto % de 0s ⏳\n",
    "2. Matriz de missing (missingno) ⏳\n",
    "3. Valores faltantes explícitos ✅\n",
    "4. Valores faltantes ocultos ✅\n",
    "5. Filas duplicadas ✅\n",
    "6. Drop policy_code ✅\n",
    "\n",
    "Decisiones de features\n",
    "\n",
    "7. Identificar columnas data leakage ⏳(ya estan identificadas, ahora hay que hacer drop)\n",
    "8. Definir estrategia second applicant (flag + drop cols) ⏳\n",
    "9. Identificar columnas ID / no predictivas ⏳(ya estan identificadas, ahora hay que hacer drop)\n",
    "10. Definir target ⏳(ya identificado)\n",
    "\n",
    "Modelado\n",
    "\n",
    "11. Crear df_model\n",
    "12. Split temporal\n",
    "13. Imputación / preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9963fc",
   "metadata": {},
   "source": [
    "We removed the pos_loan category because it represents data leaks, and also the second_applicant category, since we only need one flag and this would be application_type, which tells us if the loan was taken by a group or individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f46a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas \"post_loan\"\n",
    "post_loan_cols = analysis_dict[\"post_loan\"]\n",
    "# Lista de columnas \"second_applicant\"\n",
    "second_applicant = analysis_dict[\"second_applicant\"]\n",
    "\n",
    "cols_to_drop = post_loan_cols + second_applicant\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657267f",
   "metadata": {},
   "source": [
    "# Split train-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501892e",
   "metadata": {},
   "source": [
    "Al explorar la columna objetivo loan_status, se identificó que algunas clases tenían un número extremadamente bajo de registros (por ejemplo, solo uno o dos casos). Esto genera problemas al dividir el dataset en entrenamiento y prueba usando estratificación (stratify=y), ya que no es posible mantener la proporción de clases cuando algunas aparecen muy pocas veces.\n",
    "\n",
    "Para evitar este error y asegurar que el modelo pueda generalizar correctamente, se filtraron estas clases minoritarias, manteniendo únicamente las clases con un número suficiente de observaciones. Esto permite realizar un train-test split estratificado seguro y garantiza que tanto el conjunto de entrenamiento como el de prueba tengan representatividad adecuada de cada clase relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463071b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la frecuencia de cada clase\n",
    "print(df['loan_status'].value_counts())\n",
    "\n",
    "# Mantener solo clases frecuentes \n",
    "frequent_classes = df['loan_status'].value_counts()[df['loan_status'].value_counts() > 5].index\n",
    "df = df[df['loan_status'].isin(frequent_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tu DataFrame se llama df\n",
    "X = df.drop(columns=['loan_status'])  # Variables predictoras\n",
    "y = df['loan_status']                 # Target\n",
    "\n",
    "# Split 80% train / 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e94be",
   "metadata": {},
   "source": [
    "## Codificacion columnas tipo object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5db6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09280298",
   "metadata": {},
   "source": [
    "Manejo de fechas\n",
    "\n",
    "Las columnas de tipo fecha (issue_d, earliest_cr_line, last_pymnt_d, etc.) no se pueden usar directamente en un modelo. Por eso:\n",
    "\n",
    "- Convertimos las columnas a tipo datetime.\n",
    "\n",
    "- Creamos features derivadas, como antigüedad del crédito (credit_age) en años, que pueden ser más útiles para el modelo que la fecha cruda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', \n",
    "             'next_pymnt_d', 'last_credit_pull_d', \n",
    "             'payment_plan_start_date', 'debt_settlement_flag_date']\n",
    "\n",
    "for col in date_cols:\n",
    "    X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "    X_test[col] = pd.to_datetime(X_test[col], errors='coerce')\n",
    "\n",
    "# antigüedad del crédito\n",
    "X_train['credit_age'] = (pd.to_datetime('today') - X_train['earliest_cr_line']).dt.days / 365\n",
    "X_test['credit_age'] = (pd.to_datetime('today') - X_test['earliest_cr_line']).dt.days / 365"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d90461",
   "metadata": {},
   "source": [
    "Codificación de variables categóricas (One-hot eficiente)\n",
    "\n",
    "- a) Variables nominales (sin orden) – One-hot eficiente\n",
    "\n",
    "Usamos drop='first' para evitar multicolinealidad (importante en regresión logística)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f402b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['grade', 'sub_grade', 'home_ownership', \n",
    "                    'verification_status', 'purpose', \n",
    "                    'initial_list_status', 'application_type', \n",
    "                    'disbursement_method']\n",
    "\n",
    "# One-hot encoder eliminando la primera categoría\n",
    "ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Ajustamos solo con train\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "# Transformamos train y test\n",
    "X_train_encoded = pd.DataFrame(ohe.transform(X_train[categorical_cols]), \n",
    "                               columns=ohe.get_feature_names_out(categorical_cols), \n",
    "                               index=X_train.index)\n",
    "\n",
    "X_test_encoded = pd.DataFrame(ohe.transform(X_test[categorical_cols]), \n",
    "                              columns=ohe.get_feature_names_out(categorical_cols), \n",
    "                              index=X_test.index)\n",
    "\n",
    "# Reemplazamos las columnas originales por las codificadas\n",
    "X_train = X_train.drop(columns=categorical_cols).join(X_train_encoded)\n",
    "X_test = X_test.drop(columns=categorical_cols).join(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b95d2",
   "metadata": {},
   "source": [
    "- b) Variables ordinales (con orden)\n",
    "\n",
    "El orden es relevante, por eso asignamos números manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_map = {\n",
    "    '< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3,\n",
    "    '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7,\n",
    "    '8 years': 8, '9 years': 9, '10+ years': 10\n",
    "}\n",
    "\n",
    "X_train['emp_length'] = X_train['emp_length'].map(emp_length_map)\n",
    "X_test['emp_length'] = X_test['emp_length'].map(emp_length_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f6e13",
   "metadata": {},
   "source": [
    "Eliminación de texto libre (desc)\n",
    "\n",
    "El texto libre no aporta información estructurada inmediata y procesarlo requeriría NLP. Para mantener un modelo interpretable y eficiente, se eliminó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['desc'])\n",
    "X_test = X_test.drop(columns=['desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df479717",
   "metadata": {},
   "source": [
    "Manejo de columnas binarias\n",
    "\n",
    "Convertimos y/n a 1/0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['pymnt_plan', 'debt_settlement_flag']\n",
    "for col in binary_cols:\n",
    "    X_train[col] = X_train[col].map({'y': 1, 'n': 0})\n",
    "    X_test[col] = X_test[col].map({'y': 1, 'n': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar los tipos de todas las columnas\n",
    "print(X_train.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que siguen siendo object\n",
    "object_cols = X_train.select_dtypes(include='object').columns\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e475fa",
   "metadata": {},
   "source": [
    "Manejo de columnas tipo object restantes\n",
    "\n",
    "Al revisar el dataset tras la limpieza y codificación inicial, quedaron algunas columnas tipo object: term, emp_title, url, title, zip_code y addr_state.\n",
    "\n",
    "Estas columnas no se eliminaron en los pasos anteriores porque algunas requerían transformaciones específicas para ser útiles en el modelo, mientras que otras podían eliminarse para mantener interpretabilidad y eficiencia:\n",
    "\n",
    "- term: contiene la duración del préstamo como texto (ej. \"36 months\"). Se convirtió a un valor numérico en meses para que el modelo pueda utilizarlo directamente.\n",
    "\n",
    "- emp_title, url, title: columnas de texto libre con demasiadas categorías únicas y sin estructura clara. Se eliminaron para simplificar el modelo y mantener su interpretabilidad.\n",
    "\n",
    "- zip_code: originalmente un código postal completo, se redujo a los primeros 3 dígitos y se codificó numéricamente mediante OrdinalEncoder, manejando correctamente los códigos nuevos que aparezcan en el conjunto de prueba. Esto permite conservar información geográfica sin explotar la dimensionalidad.\n",
    "\n",
    "- addr_state: contiene el estado de residencia. Como tiene pocas categorías, se codificó mediante One-hot con drop='first', generando columnas independientes que el modelo puede interpretar sin introducir redundancia.\n",
    "\n",
    "Esta revisión garantiza que todas las variables sean numéricas o codificadas correctamente, evitando errores al entrenar una regresión logística y manteniendo la interpretabilidad y eficiencia del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ef2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Convertir 'term' a número de meses (funciona si ya es int o si es string)\n",
    "# -----------------------------\n",
    "X_train['term'] = X_train['term'].astype(str).str.replace(' months','').astype(int)\n",
    "X_test['term'] = X_test['term'].astype(str).str.replace(' months','').astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Eliminar columnas de texto libre irrelevantes\n",
    "# -----------------------------\n",
    "cols_to_drop = ['emp_title', 'url', 'title', 'desc']\n",
    "X_train = X_train.drop(columns=[c for c in cols_to_drop if c in X_train.columns])\n",
    "X_test = X_test.drop(columns=[c for c in cols_to_drop if c in X_test.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Manejo de zip_code (solo 3 primeros dígitos)\n",
    "# -----------------------------\n",
    "X_train['zip_code'] = X_train['zip_code'].astype(str).str[:3]\n",
    "X_test['zip_code'] = X_test['zip_code'].astype(str).str[:3]\n",
    "\n",
    "ord_enc_zip = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train[['zip_code']] = ord_enc_zip.fit_transform(X_train[['zip_code']])\n",
    "X_test[['zip_code']] = ord_enc_zip.transform(X_test[['zip_code']])\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ One-hot encoding de addr_state (pocas categorías)\n",
    "# -----------------------------\n",
    "if 'addr_state' in X_train.columns:\n",
    "    ohe_state = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "    ohe_state.fit(X_train[['addr_state']])\n",
    "\n",
    "    X_train_state = pd.DataFrame(ohe_state.transform(X_train[['addr_state']]),\n",
    "                                 columns=ohe_state.get_feature_names_out(['addr_state']),\n",
    "                                 index=X_train.index)\n",
    "\n",
    "    X_test_state = pd.DataFrame(ohe_state.transform(X_test[['addr_state']]),\n",
    "                                columns=ohe_state.get_feature_names_out(['addr_state']),\n",
    "                                index=X_test.index)\n",
    "\n",
    "    X_train = X_train.drop(columns=['addr_state']).join(X_train_state)\n",
    "    X_test = X_test.drop(columns=['addr_state']).join(X_test_state)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Convertir columnas datetime a métricas numéricas\n",
    "# -----------------------------\n",
    "date_cols = ['earliest_cr_line', 'issue_d', 'last_pymnt_d',\n",
    "             'next_pymnt_d', 'last_credit_pull_d',\n",
    "             'payment_plan_start_date', 'debt_settlement_flag_date']\n",
    "\n",
    "for col in date_cols:\n",
    "    if col in X_train.columns:\n",
    "        # Convertir a datetime\n",
    "        X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "        X_test[col] = pd.to_datetime(X_test[col], errors='coerce')\n",
    "        \n",
    "        # Métrica numérica: días desde hoy\n",
    "        X_train[col + '_days_since'] = (pd.to_datetime('today') - X_train[col]).dt.days\n",
    "        X_test[col + '_days_since'] = (pd.to_datetime('today') - X_test[col]).dt.days\n",
    "\n",
    "# Eliminar columnas datetime originales\n",
    "X_train = X_train.drop(columns=[c for c in date_cols if c in X_train.columns])\n",
    "X_test = X_test.drop(columns=[c for c in date_cols if c in X_test.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Verificación final: no object ni datetime\n",
    "# -----------------------------\n",
    "print(\"Tipos de columnas finales X_train:\\n\", X_train.dtypes.value_counts())\n",
    "print(\"Columnas tipo object restantes:\", X_train.select_dtypes(include='object').columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2959e6",
   "metadata": {},
   "source": [
    "Tras la codificación de las columnas tipo object y la eliminación de las columnas de texto libre irrelevantes, el dataset pasó de tener 94 columnas numéricas y 26 columnas tipo object, a:\n",
    "\n",
    "215 columnas de tipo float64\n",
    "\n",
    "4 columnas de tipo int64\n",
    "\n",
    "Esto refleja que todas las variables categóricas han sido correctamente codificadas:\n",
    "\n",
    "- Las variables binarias y ordinales se mantienen como numéricas (int64 o float64).\n",
    "\n",
    "- Las variables categóricas con pocas categorías, como addr_state, se codificaron con One-hot.\n",
    "\n",
    "- Las columnas de fecha se transformaron a métricas numéricas (días transcurridos desde cada fecha), permitiendo que los modelos interpreten la información temporal.\n",
    "\n",
    "- Las variables de texto libre que no aportaban información estructurada se eliminaron (emp_title, url, title, desc).\n",
    "\n",
    "Como resultado, no quedan columnas tipo object ni datetime, asegurando que el dataset esté completamente listo para entrenar modelos de regresión logística, IsolationForest o RandomForest, evitando errores y manteniendo interpretabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b67b50",
   "metadata": {},
   "source": [
    "### Codificación del Target (loan_status) a Binario\n",
    "\n",
    "El objetivo del proyecto es predecir si un préstamo caerá en default o no. Originalmente, la columna loan_status contenía múltiples estados textuales como:\n",
    "\n",
    "'Charged Off', 'Fully Paid', 'Current', 'Late (16-30 days)', 'Late (31-120 days)', \n",
    "'Does not meet the credit policy. Status:Charged Off', 'Does not meet the credit policy. Status:Fully Paid', 'In Grace Period'\n",
    "\n",
    "\n",
    "Para simplificar el problema a clasificación binaria, se realizó la siguiente transformación:\n",
    "\n",
    "Se definieron ciertos estados como default (1):\n",
    "'Charged Off', 'Does not meet the credit policy. Status:Charged Off', 'Does not meet the credit policy. Status:Fully Paid', 'Late (16-30 days)', 'Late (31-120 days)'.\n",
    "\n",
    "Todos los demás estados se consideraron No Default (0): 'Fully Paid', 'Current', 'In Grace Period'.\n",
    "\n",
    "Esta codificación asegura que el target sea numérico y binario, compatible con modelos de clasificación como RandomForest, XGBoost o regresión logística, evitando errores por valores categóricos y manteniendo la interpretabilidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42060edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Codificar target a binario usando map\n",
    "# -----------------------------\n",
    "default_labels = {\n",
    "    'Charged Off': 1,\n",
    "    'Does not meet the credit policy. Status:Charged Off': 1,\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 1,\n",
    "    'Late (16-30 days)': 1,\n",
    "    'Late (31-120 days)': 1,\n",
    "    'Fully Paid': 0,\n",
    "    'Current': 0,\n",
    "    'In Grace Period': 0  # puedes ajustar según tu criterio\n",
    "}\n",
    "\n",
    "y_train = y_train.map(default_labels)\n",
    "y_test = y_test.map(default_labels)\n",
    "\n",
    "# Revisar conteo de clases\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62072e4",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = IsolationForest(random_state=123)\n",
    "df_num = X_train.copy()\n",
    "df_num['outlier_flag'] = iso.fit_predict(X_train)\n",
    "df_num['outlier_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataset de entrenamiento\n",
    "\n",
    "df_train_clean = X_train.copy()\n",
    "\n",
    "#  Aplicar IsolationForest para detectar outliers\n",
    "iso = IsolationForest(random_state=123, contamination='auto')\n",
    "df_train_clean['outlier_flag'] = iso.fit_predict(df_train_clean)\n",
    "\n",
    "\n",
    "# Eliminar registros considerados outliers (-1)\n",
    "\n",
    "df_train_clean = df_train_clean[df_train_clean['outlier_flag'] == 1]\n",
    "\n",
    "\n",
    "# Eliminar la columna outlier_flag si no se va a usar como característica\n",
    "df_train_clean = df_train_clean.drop(columns=['outlier_flag'])\n",
    "y_train_clean = y_train.loc[df_train_clean.index]\n",
    "\n",
    "print(\"Número de registros después de eliminar outliers:\", df_train_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7aaa4",
   "metadata": {},
   "source": [
    "Se aplicó IsolationForest para identificar registros atípicos en el dataset de entrenamiento. Los outliers son puntos que presentan patrones muy diferentes al resto de los datos y podrían distorsionar los resultados de modelos sensibles, como la regresión logística.\n",
    "\n",
    "Tras la detección, se eliminaron los 45 registros considerados outliers de un total de 153,846, lo que representa menos del 0.03% del dataset. Esta eliminación asegura que el modelo se entrene sobre datos consistentes, manteniendo la interpretabilidad y evitando que valores extremos afecten los coeficientes.\n",
    "\n",
    "El dataset resultante conserva prácticamente toda la información original, pero más \"limpio\", garantizando una base sólida para el entrenamiento de modelos de regresión y otros algoritmos supervisados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8c425",
   "metadata": {},
   "source": [
    "# Seleccion de caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed33ffe",
   "metadata": {},
   "source": [
    "## Método 1 Feature importance de RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de RF\n",
    "model = RandomForestClassifier(random_state=42,n_jobs=-1).fit(df_train_clean, y_train_clean)\n",
    "# Importancia de características\n",
    "importances = model.feature_importances_/model.feature_importances_.sum()*100\n",
    "# Convertir a DataFrame\n",
    "df_rf_imp = pd.DataFrame({'feature': df_train_clean.columns,'rf_importance': importances}).sort_values(by='rf_importance', ascending=False)\n",
    "# Calculamos la importancia acumulada\n",
    "df_rf_imp['rf_importance_acum'] = df_rf_imp['rf_importance'].cumsum()\n",
    "df_rf_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71076d63",
   "metadata": {},
   "source": [
    "## Metodo 2 Permutation/Shuffle importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para esta técnica y la de shap se necesita conjunto de validación\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(df_train_clean, y_train_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajustamos el modelo\n",
    "model_xgb = XGBClassifier(objective='binary:logistic',random_state=42,use_label_encoder=False,eval_metric='logloss').fit(X_train1, y_train1)\n",
    "\n",
    "# Realizamos 10 permutaciones por cada característica (se usa neg_mean_absolute_error)\n",
    "perm = permutation_importance(model_xgb, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "df_perm_imp = pd.DataFrame({'feature': df_train_clean.columns, 'perm_imp': perm.importances_mean*100}).sort_values('perm_imp', ascending=False)\n",
    "df_perm_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1516396",
   "metadata": {},
   "source": [
    "## Metodo 3 SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4db911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "model_lgbm = lgb.LGBMClassifier(random_state=42, n_jobs=-1).fit(df_train_clean, y_train_clean)\n",
    "\n",
    "explainer = shap.Explainer(model_lgbm, X_val)   # usa el mismo X_val\n",
    "shap_vals = explainer(X_val).values\n",
    "\n",
    "imp_shap = np.abs(shap_vals).mean(axis=0)\n",
    "imp_shap_pct = imp_shap/imp_shap.sum()*100\n",
    "df_shap_imp = pd.DataFrame({\"feature\": X_val.columns, \"shap_imp\": imp_shap_pct}).sort_values('shap_imp', ascending=False)\n",
    "df_shap_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538311e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: MissForest in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.2.3)\n",
      "Requirement already satisfied: lazypredict in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.16)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (2.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (3.10.8)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (1.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from missingno) (0.13.2)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (8.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (1.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (2.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (1.5.3)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (4.6.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (3.1.3)\n",
      "Requirement already satisfied: pytest-runner in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (6.0.1)\n",
      "Requirement already satisfied: mlflow>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazypredict) (3.9.0)\n",
      "Requirement already satisfied: mlflow-skinny==3.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.9.0)\n",
      "Requirement already satisfied: mlflow-tracing==3.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.9.0)\n",
      "Requirement already satisfied: Flask-CORS<7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (6.0.2)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.1.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (1.18.3)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (46.0.4)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.4.3)\n",
      "Requirement already satisfied: huey<3,>=2.5.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (2.6.0)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (22.0.0)\n",
      "Requirement already satisfied: skops<1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (0.13.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (2.0.46)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow>=2.0.0->lazypredict) (3.0.2)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (6.2.6)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.1.2)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.82.0)\n",
      "Requirement already satisfied: fastapi<1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.128.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.1.46)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (8.7.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.39.1)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (6.33.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.12.5)\n",
      "Requirement already satisfied: python-dotenv<2,>=0.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (1.2.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.32.5)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.5.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.15.0)\n",
      "Requirement already satisfied: uvicorn<1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.40.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from click->lazypredict) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->missingno) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->missingno) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->lazypredict) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->lazypredict) (2025.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->lazypredict) (3.6.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow>=2.0.0->lazypredict) (1.3.10)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography<47,>=43.0.0->mlflow>=2.0.0->lazypredict) (2.0.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from docker<8,>=4.0.0->mlflow>=2.0.0->lazypredict) (311)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from docker<8,>=4.0.0->mlflow>=2.0.0->lazypredict) (2.6.3)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Flask<4->mlflow>=2.0.0->lazypredict) (3.1.5)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from graphene<4->mlflow>=2.0.0->lazypredict) (3.2.7)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from graphene<4->mlflow>=2.0.0->lazypredict) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.17.0)\n",
      "Requirement already satisfied: prettytable>=3.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from skops<1->mlflow>=2.0.0->lazypredict) (3.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.0.0->lazypredict) (3.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow>=2.0.0->lazypredict) (3.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.48.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.60b1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (from prettytable>=3.9->skops<1->mlflow>=2.0.0->lazypredict) (0.5.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (2026.1.4)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from uvicorn<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (4.12.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow>=2.0.0->lazypredict) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install missingno MissForest lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57dee29",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "El DataFrame df se crea a partir de la tabla basa_datos_pripal de la base de datos credit_scoring.db. El archivo no se incluye en el repositorio debido a su tamaño; la generación de la base de datos a partir del CSV original y la creación de la tabla se explica detalladamente en el notebook data-collection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce8220",
   "metadata": {},
   "source": [
    "Descriptive analysis:\n",
    "In this part of the project, we begin exploring the dataset created from the initial information obtained from the LendingClub dataset (Kaggle). The objective of this stage is to describe and understand the structure of the data, the variables and their types, their distributions, skewness, and the presence of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79466822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este df existen 192309 filas y 157 columnas\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(r\"C:\\Users\\User\\Documents\\GITHUB\\final_project_creditscoring\\Data\\credit_scoring.db\")\n",
    "df = pd.read_sql(\"SELECT * FROM main_table\", conn)\n",
    "conn.close()\n",
    "n_rows,n_cols = df.shape\n",
    "\n",
    "print(f'En este df existen {n_rows} filas y {n_cols} columnas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408875a",
   "metadata": {},
   "source": [
    "With the analysis below, we can understand that the dataset contains a large number of numerical variables, along with several categorical features represented as object types. This initial inspection highlights the need for feature selection and type handling in later stages. Now we are proceeding with a list of each type of column to identify possible issues in data types (such as date columns that are objects or numerical data that is shown as objects, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4362b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype\n",
       "float64    118\n",
       "object      38\n",
       "int64        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_types = df.dtypes.reset_index().rename(\n",
    "    columns={'index': 'column_name', 0: 'dtype'}\n",
    ")\n",
    "\n",
    "cols_types['dtype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe48e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>member_id</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>term</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>grade</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>emp_title</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>emp_length</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>home_ownership</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>verification_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>issue_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>loan_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pymnt_plan</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>url</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>desc</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>purpose</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>title</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>zip_code</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>addr_state</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>earliest_cr_line</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>initial_list_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>last_pymnt_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>next_pymnt_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>last_credit_pull_d</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>application_type</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>verification_status_joint</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>sec_app_earliest_cr_line</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>hardship_flag</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>hardship_type</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>hardship_reason</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>hardship_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>hardship_start_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>hardship_end_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>payment_plan_start_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>hardship_loan_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>disbursement_method</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>debt_settlement_flag</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>debt_settlement_flag_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>settlement_status</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>settlement_date</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   column_name   dtype\n",
       "1                    member_id  object\n",
       "5                         term  object\n",
       "8                        grade  object\n",
       "9                    sub_grade  object\n",
       "10                   emp_title  object\n",
       "11                  emp_length  object\n",
       "12              home_ownership  object\n",
       "14         verification_status  object\n",
       "15                     issue_d  object\n",
       "16                 loan_status  object\n",
       "17                  pymnt_plan  object\n",
       "18                         url  object\n",
       "19                        desc  object\n",
       "20                     purpose  object\n",
       "21                       title  object\n",
       "22                    zip_code  object\n",
       "23                  addr_state  object\n",
       "26            earliest_cr_line  object\n",
       "37         initial_list_status  object\n",
       "47                last_pymnt_d  object\n",
       "49                next_pymnt_d  object\n",
       "50          last_credit_pull_d  object\n",
       "56            application_type  object\n",
       "59   verification_status_joint  object\n",
       "118   sec_app_earliest_cr_line  object\n",
       "128              hardship_flag  object\n",
       "129              hardship_type  object\n",
       "130            hardship_reason  object\n",
       "131            hardship_status  object\n",
       "134        hardship_start_date  object\n",
       "135          hardship_end_date  object\n",
       "136    payment_plan_start_date  object\n",
       "139       hardship_loan_status  object\n",
       "143        disbursement_method  object\n",
       "144       debt_settlement_flag  object\n",
       "145  debt_settlement_flag_date  object\n",
       "146          settlement_status  object\n",
       "147            settlement_date  object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_types[cols_types['dtype'] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               column_name   dtype\n",
      "                                 loan_amnt float64\n",
      "                               funded_amnt float64\n",
      "                           funded_amnt_inv float64\n",
      "                                  int_rate float64\n",
      "                               installment float64\n",
      "                                annual_inc float64\n",
      "                                       dti float64\n",
      "                               delinq_2yrs float64\n",
      "                            fico_range_low float64\n",
      "                           fico_range_high float64\n",
      "                            inq_last_6mths float64\n",
      "                    mths_since_last_delinq float64\n",
      "                    mths_since_last_record float64\n",
      "                                  open_acc float64\n",
      "                                   pub_rec float64\n",
      "                                 revol_bal float64\n",
      "                                revol_util float64\n",
      "                                 total_acc float64\n",
      "                                 out_prncp float64\n",
      "                             out_prncp_inv float64\n",
      "                               total_pymnt float64\n",
      "                           total_pymnt_inv float64\n",
      "                           total_rec_prncp float64\n",
      "                             total_rec_int float64\n",
      "                        total_rec_late_fee float64\n",
      "                                recoveries float64\n",
      "                   collection_recovery_fee float64\n",
      "                           last_pymnt_amnt float64\n",
      "                      last_fico_range_high float64\n",
      "                       last_fico_range_low float64\n",
      "                collections_12_mths_ex_med float64\n",
      "               mths_since_last_major_derog float64\n",
      "                               policy_code float64\n",
      "                          annual_inc_joint float64\n",
      "                                 dti_joint float64\n",
      "                            acc_now_delinq float64\n",
      "                              tot_coll_amt float64\n",
      "                               tot_cur_bal float64\n",
      "                               open_acc_6m float64\n",
      "                               open_act_il float64\n",
      "                               open_il_12m float64\n",
      "                               open_il_24m float64\n",
      "                        mths_since_rcnt_il float64\n",
      "                              total_bal_il float64\n",
      "                                   il_util float64\n",
      "                               open_rv_12m float64\n",
      "                               open_rv_24m float64\n",
      "                                max_bal_bc float64\n",
      "                                  all_util float64\n",
      "                          total_rev_hi_lim float64\n",
      "                                    inq_fi float64\n",
      "                               total_cu_tl float64\n",
      "                              inq_last_12m float64\n",
      "                      acc_open_past_24mths float64\n",
      "                               avg_cur_bal float64\n",
      "                            bc_open_to_buy float64\n",
      "                                   bc_util float64\n",
      "                  chargeoff_within_12_mths float64\n",
      "                               delinq_amnt float64\n",
      "                        mo_sin_old_il_acct float64\n",
      "                      mo_sin_old_rev_tl_op float64\n",
      "                     mo_sin_rcnt_rev_tl_op float64\n",
      "                            mo_sin_rcnt_tl float64\n",
      "                                  mort_acc float64\n",
      "                      mths_since_recent_bc float64\n",
      "                  mths_since_recent_bc_dlq float64\n",
      "                     mths_since_recent_inq float64\n",
      "            mths_since_recent_revol_delinq float64\n",
      "                     num_accts_ever_120_pd float64\n",
      "                            num_actv_bc_tl float64\n",
      "                           num_actv_rev_tl float64\n",
      "                               num_bc_sats float64\n",
      "                                 num_bc_tl float64\n",
      "                                 num_il_tl float64\n",
      "                             num_op_rev_tl float64\n",
      "                             num_rev_accts float64\n",
      "                       num_rev_tl_bal_gt_0 float64\n",
      "                                  num_sats float64\n",
      "                          num_tl_120dpd_2m float64\n",
      "                              num_tl_30dpd float64\n",
      "                        num_tl_90g_dpd_24m float64\n",
      "                        num_tl_op_past_12m float64\n",
      "                            pct_tl_nvr_dlq float64\n",
      "                          percent_bc_gt_75 float64\n",
      "                      pub_rec_bankruptcies float64\n",
      "                                 tax_liens float64\n",
      "                           tot_hi_cred_lim float64\n",
      "                         total_bal_ex_mort float64\n",
      "                            total_bc_limit float64\n",
      "                total_il_high_credit_limit float64\n",
      "                           revol_bal_joint float64\n",
      "                    sec_app_fico_range_low float64\n",
      "                   sec_app_fico_range_high float64\n",
      "                    sec_app_inq_last_6mths float64\n",
      "                          sec_app_mort_acc float64\n",
      "                          sec_app_open_acc float64\n",
      "                        sec_app_revol_util float64\n",
      "                       sec_app_open_act_il float64\n",
      "                     sec_app_num_rev_accts float64\n",
      "          sec_app_chargeoff_within_12_mths float64\n",
      "        sec_app_collections_12_mths_ex_med float64\n",
      "       sec_app_mths_since_last_major_derog float64\n",
      "                             deferral_term float64\n",
      "                           hardship_amount float64\n",
      "                           hardship_length float64\n",
      "                              hardship_dpd float64\n",
      "orig_projected_additional_accrued_interest float64\n",
      "            hardship_payoff_balance_amount float64\n",
      "              hardship_last_payment_amount float64\n",
      "                         settlement_amount float64\n",
      "                     settlement_percentage float64\n",
      "                           settlement_term float64\n",
      "                             inflation_cpi float64\n",
      "                                       gdp float64\n",
      "                         unemployment_rate float64\n",
      "                            fed_funds_rate float64\n",
      "                                       pce float64\n",
      "                     total_consumer_credit float64\n"
     ]
    }
   ],
   "source": [
    "print(cols_types[cols_types['dtype'] == 'float64'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5a2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_name  dtype\n",
       "0          id  int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_types[cols_types['dtype'] == 'int64']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9447a",
   "metadata": {},
   "source": [
    "Revision of constant columns: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4188ff1",
   "metadata": {},
   "source": [
    "The following code initially didn’t specify dropna=False, which made it show a few columns as constants. This led us to investigate what was happening and whether we were working with the right dataframe. However, this mistake was enlightening, as it helped us identify possible *data leakage variables, such as: hardship_type, deferral_term, and hardship_length.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a9960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                            192309\n",
      "member_id                                          1\n",
      "loan_amnt                                       1456\n",
      "funded_amnt                                     1457\n",
      "funded_amnt_inv                                 4943\n",
      "term                                               2\n",
      "int_rate                                         500\n",
      "installment                                    42329\n",
      "grade                                              7\n",
      "sub_grade                                         35\n",
      "emp_title                                      92693\n",
      "emp_length                                        12\n",
      "home_ownership                                     6\n",
      "annual_inc                                     15650\n",
      "verification_status                                3\n",
      "issue_d                                          103\n",
      "loan_status                                        9\n",
      "pymnt_plan                                         2\n",
      "url                                           192309\n",
      "desc                                           42236\n",
      "purpose                                           14\n",
      "title                                          25801\n",
      "zip_code                                         892\n",
      "addr_state                                        50\n",
      "dti                                             5070\n",
      "delinq_2yrs                                       22\n",
      "earliest_cr_line                                 665\n",
      "fico_range_low                                    38\n",
      "fico_range_high                                   38\n",
      "inq_last_6mths                                     9\n",
      "mths_since_last_delinq                           131\n",
      "mths_since_last_record                           122\n",
      "open_acc                                          64\n",
      "pub_rec                                           15\n",
      "revol_bal                                      45692\n",
      "revol_util                                      1178\n",
      "total_acc                                        113\n",
      "initial_list_status                                2\n",
      "out_prncp                                      35706\n",
      "out_prncp_inv                                  36213\n",
      "total_pymnt                                   183549\n",
      "total_pymnt_inv                               170864\n",
      "total_rec_prncp                                61547\n",
      "total_rec_int                                 147972\n",
      "total_rec_late_fee                              4558\n",
      "recoveries                                     18125\n",
      "collection_recovery_fee                        16373\n",
      "last_pymnt_d                                     106\n",
      "last_pymnt_amnt                               119856\n",
      "next_pymnt_d                                      69\n",
      "last_credit_pull_d                               107\n",
      "last_fico_range_high                              72\n",
      "last_fico_range_low                               71\n",
      "collections_12_mths_ex_med                         6\n",
      "mths_since_last_major_derog                      143\n",
      "policy_code                                        1\n",
      "application_type                                   2\n",
      "annual_inc_joint                                1632\n",
      "dti_joint                                       2662\n",
      "verification_status_joint                          4\n",
      "acc_now_delinq                                     5\n",
      "tot_coll_amt                                    4350\n",
      "tot_cur_bal                                   112510\n",
      "open_acc_6m                                       15\n",
      "open_act_il                                       42\n",
      "open_il_12m                                       11\n",
      "open_il_24m                                       20\n",
      "mths_since_rcnt_il                               253\n",
      "total_bal_il                                   43950\n",
      "il_util                                          190\n",
      "open_rv_12m                                       22\n",
      "open_rv_24m                                       36\n",
      "max_bal_bc                                     16973\n",
      "all_util                                         150\n",
      "total_rev_hi_lim                                6580\n",
      "inq_fi                                            21\n",
      "total_cu_tl                                       42\n",
      "inq_last_12m                                      35\n",
      "acc_open_past_24mths                              42\n",
      "avg_cur_bal                                    41008\n",
      "bc_open_to_buy                                 36765\n",
      "bc_util                                         1182\n",
      "chargeoff_within_12_mths                           7\n",
      "delinq_amnt                                      332\n",
      "mo_sin_old_il_acct                               442\n",
      "mo_sin_old_rev_tl_op                             663\n",
      "mo_sin_rcnt_rev_tl_op                            216\n",
      "mo_sin_rcnt_tl                                   145\n",
      "mort_acc                                          27\n",
      "mths_since_recent_bc                             369\n",
      "mths_since_recent_bc_dlq                         132\n",
      "mths_since_recent_inq                             27\n",
      "mths_since_recent_revol_delinq                   140\n",
      "num_accts_ever_120_pd                             31\n",
      "num_actv_bc_tl                                    29\n",
      "num_actv_rev_tl                                   43\n",
      "num_bc_sats                                       43\n",
      "num_bc_tl                                         54\n",
      "num_il_tl                                         86\n",
      "num_op_rev_tl                                     58\n",
      "num_rev_accts                                     87\n",
      "num_rev_tl_bal_gt_0                               39\n",
      "num_sats                                          64\n",
      "num_tl_120dpd_2m                                   5\n",
      "num_tl_30dpd                                       6\n",
      "num_tl_90g_dpd_24m                                22\n",
      "num_tl_op_past_12m                                24\n",
      "pct_tl_nvr_dlq                                   477\n",
      "percent_bc_gt_75                                 172\n",
      "pub_rec_bankruptcies                               9\n",
      "tax_liens                                         15\n",
      "tot_hi_cred_lim                               109217\n",
      "total_bal_ex_mort                              82607\n",
      "total_bc_limit                                  5175\n",
      "total_il_high_credit_limit                     69760\n",
      "revol_bal_joint                                 4996\n",
      "sec_app_fico_range_low                            62\n",
      "sec_app_fico_range_high                           62\n",
      "sec_app_earliest_cr_line                         477\n",
      "sec_app_inq_last_6mths                             8\n",
      "sec_app_mort_acc                                  14\n",
      "sec_app_open_acc                                  51\n",
      "sec_app_revol_util                              1013\n",
      "sec_app_open_act_il                               31\n",
      "sec_app_num_rev_accts                             64\n",
      "sec_app_chargeoff_within_12_mths                  12\n",
      "sec_app_collections_12_mths_ex_med                 8\n",
      "sec_app_mths_since_last_major_derog              100\n",
      "hardship_flag                                      2\n",
      "hardship_type                                      2\n",
      "hardship_reason                                   10\n",
      "hardship_status                                    4\n",
      "deferral_term                                      2\n",
      "hardship_amount                                  590\n",
      "hardship_start_date                               27\n",
      "hardship_end_date                                 26\n",
      "payment_plan_start_date                           27\n",
      "hardship_length                                    2\n",
      "hardship_dpd                                      33\n",
      "hardship_loan_status                               5\n",
      "orig_projected_additional_accrued_interest       453\n",
      "hardship_payoff_balance_amount                   597\n",
      "hardship_last_payment_amount                     590\n",
      "disbursement_method                                2\n",
      "debt_settlement_flag                               2\n",
      "debt_settlement_flag_date                         78\n",
      "settlement_status                                  4\n",
      "settlement_date                                   86\n",
      "settlement_amount                               2291\n",
      "settlement_percentage                            449\n",
      "settlement_term                                   29\n",
      "inflation_cpi                                    101\n",
      "gdp                                               35\n",
      "unemployment_rate                                 48\n",
      "fed_funds_rate                                    42\n",
      "pce                                              103\n",
      "total_consumer_credit                            103\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Unique values per column\n",
    "uniq = df.nunique(dropna=False)\n",
    "\n",
    "# Show results\n",
    "print(uniq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64b760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member_id      1\n",
       "policy_code    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq = df.nunique(dropna=False)\n",
    "uniq[uniq == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f5eaa",
   "metadata": {},
   "source": [
    "After the code revision, the only two variables with constant values are member_id and policy_code. It does not make much sense to have a unique value for member_id if we have almost 200k data entries, so we needed to check the exact values contained in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fa763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member_id\n",
      "None    192309\n",
      "Name: count, dtype: int64\n",
      "policy_code\n",
      "1.0    192309\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols = ['member_id', 'policy_code']\n",
    "\n",
    "for col in cols:\n",
    "    print(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26826a2e",
   "metadata": {},
   "source": [
    "Regarding policy_code, a similar situation occurs. According to the data dictionary, LendingClub has only two types of policies: publicly available (1) and new products not publicly available (2). In this dataset, only publicly available products are present. Therefore, following the same reasoning as above, policy_code is not a relevant column for the analysis.\n",
    "\n",
    "Lastly, as we are dropping member_id and policy_code because they are not predictors, we are doing the same with two other columns as well, which, even though they do not have constant values, can generate noise in the analysis: the url column and the id column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22690d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['member_id', 'policy_code','id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc3d17",
   "metadata": {},
   "source": [
    "Revision of Duplicated Rows: No duplicated rows were identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f3658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c81a6",
   "metadata": {},
   "source": [
    "Revision of Duplicated Columns: Two variables (deferral_term and hardship_length) were found to be exact duplicates, containing identical values across all observations. Both variables are related to post-loan hardship events (we previously identified them as potential data leakers) and will therefore be excluded from the modeling stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2471a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                                     False\n",
       "funded_amnt                                   False\n",
       "funded_amnt_inv                               False\n",
       "term                                          False\n",
       "int_rate                                      False\n",
       "installment                                   False\n",
       "grade                                         False\n",
       "sub_grade                                     False\n",
       "emp_title                                     False\n",
       "emp_length                                    False\n",
       "home_ownership                                False\n",
       "annual_inc                                    False\n",
       "verification_status                           False\n",
       "issue_d                                       False\n",
       "loan_status                                   False\n",
       "pymnt_plan                                    False\n",
       "url                                           False\n",
       "desc                                          False\n",
       "purpose                                       False\n",
       "title                                         False\n",
       "zip_code                                      False\n",
       "addr_state                                    False\n",
       "dti                                           False\n",
       "delinq_2yrs                                   False\n",
       "earliest_cr_line                              False\n",
       "fico_range_low                                False\n",
       "fico_range_high                               False\n",
       "inq_last_6mths                                False\n",
       "mths_since_last_delinq                        False\n",
       "mths_since_last_record                        False\n",
       "open_acc                                      False\n",
       "pub_rec                                       False\n",
       "revol_bal                                     False\n",
       "revol_util                                    False\n",
       "total_acc                                     False\n",
       "initial_list_status                           False\n",
       "out_prncp                                     False\n",
       "out_prncp_inv                                 False\n",
       "total_pymnt                                   False\n",
       "total_pymnt_inv                               False\n",
       "total_rec_prncp                               False\n",
       "total_rec_int                                 False\n",
       "total_rec_late_fee                            False\n",
       "recoveries                                    False\n",
       "collection_recovery_fee                       False\n",
       "last_pymnt_d                                  False\n",
       "last_pymnt_amnt                               False\n",
       "next_pymnt_d                                  False\n",
       "last_credit_pull_d                            False\n",
       "last_fico_range_high                          False\n",
       "last_fico_range_low                           False\n",
       "collections_12_mths_ex_med                    False\n",
       "mths_since_last_major_derog                   False\n",
       "application_type                              False\n",
       "annual_inc_joint                              False\n",
       "dti_joint                                     False\n",
       "verification_status_joint                     False\n",
       "acc_now_delinq                                False\n",
       "tot_coll_amt                                  False\n",
       "tot_cur_bal                                   False\n",
       "open_acc_6m                                   False\n",
       "open_act_il                                   False\n",
       "open_il_12m                                   False\n",
       "open_il_24m                                   False\n",
       "mths_since_rcnt_il                            False\n",
       "total_bal_il                                  False\n",
       "il_util                                       False\n",
       "open_rv_12m                                   False\n",
       "open_rv_24m                                   False\n",
       "max_bal_bc                                    False\n",
       "all_util                                      False\n",
       "total_rev_hi_lim                              False\n",
       "inq_fi                                        False\n",
       "total_cu_tl                                   False\n",
       "inq_last_12m                                  False\n",
       "acc_open_past_24mths                          False\n",
       "avg_cur_bal                                   False\n",
       "bc_open_to_buy                                False\n",
       "bc_util                                       False\n",
       "chargeoff_within_12_mths                      False\n",
       "delinq_amnt                                   False\n",
       "mo_sin_old_il_acct                            False\n",
       "mo_sin_old_rev_tl_op                          False\n",
       "mo_sin_rcnt_rev_tl_op                         False\n",
       "mo_sin_rcnt_tl                                False\n",
       "mort_acc                                      False\n",
       "mths_since_recent_bc                          False\n",
       "mths_since_recent_bc_dlq                      False\n",
       "mths_since_recent_inq                         False\n",
       "mths_since_recent_revol_delinq                False\n",
       "num_accts_ever_120_pd                         False\n",
       "num_actv_bc_tl                                False\n",
       "num_actv_rev_tl                               False\n",
       "num_bc_sats                                   False\n",
       "num_bc_tl                                     False\n",
       "num_il_tl                                     False\n",
       "num_op_rev_tl                                 False\n",
       "num_rev_accts                                 False\n",
       "num_rev_tl_bal_gt_0                           False\n",
       "num_sats                                      False\n",
       "num_tl_120dpd_2m                              False\n",
       "num_tl_30dpd                                  False\n",
       "num_tl_90g_dpd_24m                            False\n",
       "num_tl_op_past_12m                            False\n",
       "pct_tl_nvr_dlq                                False\n",
       "percent_bc_gt_75                              False\n",
       "pub_rec_bankruptcies                          False\n",
       "tax_liens                                     False\n",
       "tot_hi_cred_lim                               False\n",
       "total_bal_ex_mort                             False\n",
       "total_bc_limit                                False\n",
       "total_il_high_credit_limit                    False\n",
       "revol_bal_joint                               False\n",
       "sec_app_fico_range_low                        False\n",
       "sec_app_fico_range_high                       False\n",
       "sec_app_earliest_cr_line                      False\n",
       "sec_app_inq_last_6mths                        False\n",
       "sec_app_mort_acc                              False\n",
       "sec_app_open_acc                              False\n",
       "sec_app_revol_util                            False\n",
       "sec_app_open_act_il                           False\n",
       "sec_app_num_rev_accts                         False\n",
       "sec_app_chargeoff_within_12_mths              False\n",
       "sec_app_collections_12_mths_ex_med            False\n",
       "sec_app_mths_since_last_major_derog           False\n",
       "hardship_flag                                 False\n",
       "hardship_type                                 False\n",
       "hardship_reason                               False\n",
       "hardship_status                               False\n",
       "deferral_term                                  True\n",
       "hardship_amount                               False\n",
       "hardship_start_date                           False\n",
       "hardship_end_date                             False\n",
       "payment_plan_start_date                       False\n",
       "hardship_length                                True\n",
       "hardship_dpd                                  False\n",
       "hardship_loan_status                          False\n",
       "orig_projected_additional_accrued_interest    False\n",
       "hardship_payoff_balance_amount                False\n",
       "hardship_last_payment_amount                  False\n",
       "disbursement_method                           False\n",
       "debt_settlement_flag                          False\n",
       "debt_settlement_flag_date                     False\n",
       "settlement_status                             False\n",
       "settlement_date                               False\n",
       "settlement_amount                             False\n",
       "settlement_percentage                         False\n",
       "settlement_term                               False\n",
       "inflation_cpi                                 False\n",
       "gdp                                           False\n",
       "unemployment_rate                             False\n",
       "fed_funds_rate                                False\n",
       "pce                                           False\n",
       "total_consumer_credit                         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T.duplicated().sum()\n",
    "df.T.duplicated(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf6598",
   "metadata": {},
   "source": [
    "Missing values: \n",
    "We identified columns with a high percentage of missing values, so we proceeded to define a missing threshold of 50%, where variables with more than 50% missing values will be considered for exclusion from the modeling stage. However, first we must evaluate them on a case-by-case basis to understand if any of those variables are conceptually important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isna().mean()*100\n",
    "missing[missing>0]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0\n",
    "\n",
    "high_missing_cols = missing[missing >= missing_threshold]\n",
    "print(high_missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "(missing > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ca262",
   "metadata": {},
   "source": [
    "Now we have to identify other missing values and audit them to understand how we should treat each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff185d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify other missing values\n",
    "cat_col = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in cat_col: \n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a23a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faltantes = df.replace(['None'],np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f5f49",
   "metadata": {},
   "source": [
    "Columns with more than 50% missing values were manually reviewed and classified into post-loan variables, second-applicant features, structurally missing variables, and late-reported behavioral features based on domain knowledge and data documentation. No features were removed at this stage; the analysis documents decisions to be applied during model preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a69d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_column(col):\n",
    "    if col.startswith((\"hardship\", \"settlement\", \"deferral\")):\n",
    "        return \"post_loan\"\n",
    "    if col.startswith(\"sec_app\") or col.endswith(\"_joint\"):\n",
    "        return \"second_applicant\"\n",
    "    if col.startswith(\"mths_since\"):\n",
    "        return \"structural_missing\"\n",
    "    return \"other\"\n",
    "\n",
    "categories = [\"post_loan\", \"second_applicant\", \"structural_missing\"]\n",
    "analysis_dict = {cat: [col for col in df.columns if classify_column(col) == cat] for cat in categories}\n",
    "\n",
    "print(\"🔍 --- STARTING COLUMN CATEGORIZATION ANALYSIS --- 🔍\\n\")\n",
    "\n",
    "for category, cols in analysis_dict.items():\n",
    "    print(f\"📁 CATEGORY: {category.upper()}\")\n",
    "    if not cols:\n",
    "        print(\"   ❌ No columns found in this category.\\n\")\n",
    "    else:\n",
    "        print(f\"   ✅ Found {len(cols)} columns.\")\n",
    "        missing_stats = df[cols].isnull().mean() * 100\n",
    "        print(missing_stats.sort_values(ascending=False).to_string())\n",
    "        print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "print(\"🚀 --- ANALYSIS COMPLETE --- 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8177f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column checker: to be able to quickly check the characteristics of a column and its type\n",
    "\n",
    "target_col = 'loan_status'  \n",
    "\n",
    "col_type = df[target_col].dtype\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Count': df[target_col].value_counts(dropna=False),\n",
    "    'Percentage (%)': df[target_col].value_counts(dropna=False, normalize=True) * 100\n",
    "})\n",
    "\n",
    "print(f\"Content Analysis for: {target_col.upper()}\")\n",
    "print(summary)\n",
    "print('---'*30)\n",
    "print(f\"Data Type: {col_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classified_cols = (analysis_dict['post_loan'] + \n",
    "                      analysis_dict['second_applicant'] + \n",
    "                      analysis_dict['structural_missing'])\n",
    "\n",
    "other_cols = [col for col in df.columns if col not in all_classified_cols]\n",
    "\n",
    "print(\"🔍 --- AUDITING 'OTHER' COLUMNS WITH HIGH MISSING RATIO (>40%) ---\")\n",
    "high_missing_other = df[other_cols].isnull().mean()\n",
    "high_missing_other = high_missing_other[high_missing_other > 0.4].sort_values(ascending=False)\n",
    "\n",
    "if high_missing_other.empty:\n",
    "    print(\"✅ No additional critical missing values found outside defined categories.\")\n",
    "else:\n",
    "    print(\"⚠️ Attention: The following columns also have a high missing ratio:\")\n",
    "    print(high_missing_other.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b39f43",
   "metadata": {},
   "source": [
    "1. *Post-loan variables:*\n",
    "\n",
    "These features contain information generated after loan origination, such as hardship or settlement events. Their high missingness reflects the fact that most loans do not enter these processes. Because these variables include future information relative to the credit decision, they were identified as potential sources of data leakage.\n",
    "\n",
    "Planned decision: Exclude.\n",
    "\n",
    "2. *Second-applicant variables:*\n",
    "\n",
    "These variables describe characteristics of a co-borrower in joint loan applications. The high proportion of missing values reflects that most loans involve a single applicant, meaning missing values indicate the absence of a second applicant rather than missing information.\n",
    "\n",
    "Rather than modeling the full co-borrower profile, the presence of a second applicant is captured through a binary indicator. This approach preserves potentially relevant information while avoiding additional complexity and extensive imputation.\n",
    "\n",
    "Planned decision: Create a binary flag indicating whether a loan includes a second applicant, and exclude detailed second-applicant features during model preparation.\n",
    "\n",
    "3. *Structurally missing variables:*\n",
    "\n",
    "These features represent the time since the last occurrence of negative credit events. Missing values indicate that the event has never occurred, making the missingness itself informative.\n",
    "\n",
    "Planned decision: Retain for modeling and apply a dedicated imputation strategy at a later stage.\n",
    "\n",
    "4. *Late-reported features:*\n",
    "\n",
    "These variables were introduced into the dataset at later periods and are unavailable for older loans. Missingness is driven by historical reporting limitations rather than borrower behavior.\n",
    "\n",
    "Planned decision: Evaluate after defining the temporal train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c167fa7",
   "metadata": {},
   "source": [
    "DATA CLEANING & PREPROCESSING STRATEGY\n",
    "\n",
    "Now that we have a clearer understanding of the data, we can proceed with data cleaning and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ec889",
   "metadata": {},
   "source": [
    "1. DF Backup: Create a full copy of the raw dataset to ensure data integrity and allow for easy rollbacks during the experimentation phase.\n",
    "\n",
    "2. Target Definition & Filtering: Refine the loan_status variable. We exclude ongoing loans and focus only on definitive outcomes.\n",
    "\n",
    "Default (1): Charged off, default, or late (30–120 days).\n",
    "Charged Off\n",
    "Late (31-120 days)\n",
    "Default\n",
    "Does not meet the credit policy. Status:Charged Off\n",
    "\n",
    "Non-Default (0): Fully paid.\n",
    "Fully Paid\n",
    "\n",
    "3. Leakage Removal: Drop all Post-loan variables. These features contain information only available after the credit decision has been made, which would lead to Data Leakage.\n",
    "\n",
    "4. Structural Simplification (Joint Apps): Consolidate +16 second-applicant features into a single Binary Flag (is_joint_application). This reduces dimensionality while preserving the fact that a co-borrower exists.\n",
    "\n",
    "5. Zero Ratio & Variance Analysis: Identify features with excessive sparsity. We decide whether to drop columns with near-zero variance or binarize features where the simple presence of an event (0 vs >0) is more predictive than its frequency.\n",
    "\n",
    "6. Missingness Audit (Missingno): Visualize the remaining missing values to determine the mechanism of missingness (Random vs. Structural). This dictates the final decision: drop the column (if >50% NaN) or keep it for Imputation after the Train-Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect unique values and their prevalence\n",
    "def inspect_categories(dataframe, column_list):\n",
    "    \"\"\"\n",
    "    Prints frequency and percentage distribution for categorical features.\n",
    "    \"\"\"\n",
    "    for col in column_list:\n",
    "        print(f\"\\n--- Feature: {col.upper()} ---\")\n",
    "        \n",
    "        counts = dataframe[col].value_counts(dropna=False)\n",
    "        percentages = dataframe[col].value_counts(dropna=False, normalize=True) * 100\n",
    "        \n",
    "        summary = pd.DataFrame({\n",
    "            'Count': counts,\n",
    "            'Percentage (%)': percentages.round(2)\n",
    "        })\n",
    "        \n",
    "        print(summary)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "target_cols = analysis_dict.get('other', [])\n",
    "inspect_categories(df, target_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0a0b2",
   "metadata": {},
   "source": [
    "Now we proceed with a zero ratio analysis, that has the objective of helping us decide which variables doesn't have enough information to support the model and which ones are corrupt with 0s that should be NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee92ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero Ratio Analysis\n",
    "zero_ratio = (df == 0).mean() * 100\n",
    "zero_ratio_all = zero_ratio[zero_ratio > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"ALL COLUMNS WITH ZEROS\")\n",
    "print(zero_ratio_all.to_string()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e7d88",
   "metadata": {},
   "source": [
    "Having identified the columns with higher amounts of 0s, we decided to audit them one by one and review their unique values to understand whether the 0s are informative or if they represent null/NaN values. This audit is half based on the code shown below and half on a manual review of the dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Zero Ratio again to get the target columns\n",
    "zero_ratio = (df == 0).mean() * 100\n",
    "# Define a threshold (e.g., columns with more than 50% zeros)\n",
    "high_zero_threshold = 30.0\n",
    "high_zero_cols = zero_ratio[zero_ratio > high_zero_threshold].sort_values(ascending=False).index.tolist()\n",
    "\n",
    "def audit_high_zero_columns(dataframe, column_list):\n",
    "    \"\"\"\n",
    "    Audits columns with high zero ratios to see value distribution \n",
    "    and help decide between dropping, keeping, or binarizing.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 --- AUDITING {len(column_list)} COLUMNS WITH > {high_zero_threshold}% ZEROS --- \\n\")\n",
    "    \n",
    "    for col in column_list:\n",
    "        print(f\"📊 Feature: {col.upper()}\")\n",
    "        print(f\"Zero Ratio: {zero_ratio[col]:.2f}%\")\n",
    "        \n",
    "        # Count unique values excluding zero\n",
    "        non_zero_values = dataframe[dataframe[col] != 0][col]\n",
    "        unique_counts = non_zero_values.nunique()\n",
    "        \n",
    "        print(f\"Unique values (excluding zero): {unique_counts}\")\n",
    "        \n",
    "        if unique_counts < 15:\n",
    "            # If few unique values, show frequency\n",
    "            print(\"Distribution (Top Values):\")\n",
    "            print(dataframe[col].value_counts().head(10))\n",
    "        else:\n",
    "            # If many unique values, show basic stats for non-zero data\n",
    "            print(\"Non-zero stats:\")\n",
    "            print(non_zero_values.describe()[['mean', 'min', 'max']])\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Execute the audit\n",
    "audit_high_zero_columns(df, high_zero_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274c5a5",
   "metadata": {},
   "source": [
    "The function below is intended to serve as a filter to help us quickly verify whether a column was correctly classified in the categories defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_category(column_name, mapping_dict):\n",
    "    \"\"\"\n",
    "    Checks which category a specific column belongs to based on the analysis_dict.\n",
    "    \"\"\"\n",
    "    for category, columns in mapping_dict.items():\n",
    "        if column_name in columns:\n",
    "            return category\n",
    "    return \"other (or not found)\"\n",
    "\n",
    "test_col = 'loan_status' # You can change this name to any column\n",
    "result = get_column_category(test_col, analysis_dict)\n",
    "print(f\"Verification: The column '{test_col}' is categorized as: {result.upper()}\")\n",
    "\n",
    "# 3. Batch verification (Optional)\n",
    "# List of columns you want to verify right now\n",
    "verify_list = ['sec_app_fico_range_low', 'mths_since_last_delinq', 'loan_amnt', 'settlement_term']\n",
    "\n",
    "print(\"BATCH VERIFICATION:\")\n",
    "for col in verify_list:\n",
    "    cat = get_column_category(col, analysis_dict)\n",
    "    print(f\"- {col:30} -> Category: {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d217608",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    vc = df[col].value_counts(dropna=False, normalize=True)\n",
    "    if vc.iloc[0] > 0.99:   # más del 99% el mismo valor\n",
    "        low_variance_cols.append(col)\n",
    "\n",
    "low_variance_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df.describe().T\n",
    "desc.sort_values(by='max', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02778525",
   "metadata": {},
   "source": [
    "The analysis of the maximum values reveals the presence of extreme values in some financial variables, which suggests the need to apply transformations or outlier treatment techniques in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.assign(\n",
    "    mean_median_ratio = desc['mean'] / desc['50%']\n",
    ").sort_values('mean_median_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53cd516",
   "metadata": {},
   "source": [
    "Several numerical variables present highly skewed distributions, with median values equal to zero and a small proportion of non-zero observations. This pattern is expected for count-based credit history variables. However, some highly skewed variables correspond to post-loan information and will therefore be excluded from the modeling process to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574497f4",
   "metadata": {},
   "source": [
    "EDA (cerrando)\n",
    "1. Tratamiento columnas con alto % de 0s ⏳\n",
    "2. Matriz de missing (missingno) ⏳\n",
    "3. Valores faltantes explícitos ✅\n",
    "4. Valores faltantes ocultos ✅\n",
    "5. Filas duplicadas ✅\n",
    "6. Drop policy_code ✅\n",
    "\n",
    "Decisiones de features\n",
    "\n",
    "7. Identificar columnas data leakage ⏳(ya estan identificadas, ahora hay que hacer drop)\n",
    "8. Definir estrategia second applicant (flag + drop cols) ⏳\n",
    "9. Identificar columnas ID / no predictivas ⏳(ya estan identificadas, ahora hay que hacer drop)\n",
    "10. Definir target ⏳(ya identificado)\n",
    "\n",
    "Modelado\n",
    "\n",
    "11. Crear df_model\n",
    "12. Split temporal\n",
    "13. Imputación / preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd4912",
   "metadata": {},
   "source": [
    "We removed the pos_loan category because it represents data leaks, and also the second_applicant category, since we only need one flag and this would be application_type, which tells us if the loan was taken by a group or individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aaae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas \"post_loan\"\n",
    "post_loan_cols = analysis_dict[\"post_loan\"]\n",
    "# Lista de columnas \"second_applicant\"\n",
    "second_applicant = analysis_dict[\"second_applicant\"]\n",
    "\n",
    "cols_to_drop = post_loan_cols + second_applicant\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47a363",
   "metadata": {},
   "source": [
    "# Split train-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc116f",
   "metadata": {},
   "source": [
    "Al explorar la columna objetivo loan_status, se identificó que algunas clases tenían un número extremadamente bajo de registros (por ejemplo, solo uno o dos casos). Esto genera problemas al dividir el dataset en entrenamiento y prueba usando estratificación (stratify=y), ya que no es posible mantener la proporción de clases cuando algunas aparecen muy pocas veces.\n",
    "\n",
    "Para evitar este error y asegurar que el modelo pueda generalizar correctamente, se filtraron estas clases minoritarias, manteniendo únicamente las clases con un número suficiente de observaciones. Esto permite realizar un train-test split estratificado seguro y garantiza que tanto el conjunto de entrenamiento como el de prueba tengan representatividad adecuada de cada clase relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la frecuencia de cada clase\n",
    "print(df['loan_status'].value_counts())\n",
    "\n",
    "# Mantener solo clases frecuentes \n",
    "frequent_classes = df['loan_status'].value_counts()[df['loan_status'].value_counts() > 5].index\n",
    "df = df[df['loan_status'].isin(frequent_classes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d55e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tu DataFrame se llama df\n",
    "X = df.drop(columns=['loan_status'])  # Variables predictoras\n",
    "y = df['loan_status']                 # Target\n",
    "\n",
    "# Split 80% train / 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeefb1d",
   "metadata": {},
   "source": [
    "## Codificacion columnas tipo object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186700a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='object').columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f0d2e",
   "metadata": {},
   "source": [
    "Manejo de fechas\n",
    "\n",
    "Las columnas de tipo fecha (issue_d, earliest_cr_line, last_pymnt_d, etc.) no se pueden usar directamente en un modelo. Por eso:\n",
    "\n",
    "- Convertimos las columnas a tipo datetime.\n",
    "\n",
    "- Creamos features derivadas, como antigüedad del crédito (credit_age) en años, que pueden ser más útiles para el modelo que la fecha cruda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', \n",
    "             'next_pymnt_d', 'last_credit_pull_d', \n",
    "             'payment_plan_start_date', 'debt_settlement_flag_date']\n",
    "\n",
    "for col in date_cols:\n",
    "    X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "    X_test[col] = pd.to_datetime(X_test[col], errors='coerce')\n",
    "\n",
    "# antigüedad del crédito\n",
    "X_train['credit_age'] = (pd.to_datetime('today') - X_train['earliest_cr_line']).dt.days / 365\n",
    "X_test['credit_age'] = (pd.to_datetime('today') - X_test['earliest_cr_line']).dt.days / 365\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d9621",
   "metadata": {},
   "source": [
    "Codificación de variables categóricas (One-hot eficiente)\n",
    "\n",
    "- a) Variables nominales (sin orden) – One-hot eficiente\n",
    "\n",
    "Usamos drop='first' para evitar multicolinealidad (importante en regresión logística)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['grade', 'sub_grade', 'home_ownership', \n",
    "                    'verification_status', 'purpose', \n",
    "                    'initial_list_status', 'application_type', \n",
    "                    'disbursement_method']\n",
    "\n",
    "# One-hot encoder eliminando la primera categoría\n",
    "ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Ajustamos solo con train\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "# Transformamos train y test\n",
    "X_train_encoded = pd.DataFrame(ohe.transform(X_train[categorical_cols]), \n",
    "                               columns=ohe.get_feature_names_out(categorical_cols), \n",
    "                               index=X_train.index)\n",
    "\n",
    "X_test_encoded = pd.DataFrame(ohe.transform(X_test[categorical_cols]), \n",
    "                              columns=ohe.get_feature_names_out(categorical_cols), \n",
    "                              index=X_test.index)\n",
    "\n",
    "# Reemplazamos las columnas originales por las codificadas\n",
    "X_train = X_train.drop(columns=categorical_cols).join(X_train_encoded)\n",
    "X_test = X_test.drop(columns=categorical_cols).join(X_test_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b874ec",
   "metadata": {},
   "source": [
    "- b) Variables ordinales (con orden)\n",
    "\n",
    "El orden es relevante, por eso asignamos números manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_map = {\n",
    "    '< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3,\n",
    "    '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7,\n",
    "    '8 years': 8, '9 years': 9, '10+ years': 10\n",
    "}\n",
    "\n",
    "X_train['emp_length'] = X_train['emp_length'].map(emp_length_map)\n",
    "X_test['emp_length'] = X_test['emp_length'].map(emp_length_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e04872",
   "metadata": {},
   "source": [
    "Eliminación de texto libre (desc)\n",
    "\n",
    "El texto libre no aporta información estructurada inmediata y procesarlo requeriría NLP. Para mantener un modelo interpretable y eficiente, se eliminó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['desc'])\n",
    "X_test = X_test.drop(columns=['desc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1bd79",
   "metadata": {},
   "source": [
    "Manejo de columnas binarias\n",
    "\n",
    "Convertimos y/n a 1/0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9761de",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['pymnt_plan', 'debt_settlement_flag']\n",
    "for col in binary_cols:\n",
    "    X_train[col] = X_train[col].map({'y': 1, 'n': 0})\n",
    "    X_test[col] = X_test[col].map({'y': 1, 'n': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar los tipos de todas las columnas\n",
    "print(X_train.dtypes.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que siguen siendo object\n",
    "object_cols = X_train.select_dtypes(include='object').columns\n",
    "print(object_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbee8ab",
   "metadata": {},
   "source": [
    "Manejo de columnas tipo object restantes\n",
    "\n",
    "Al revisar el dataset tras la limpieza y codificación inicial, quedaron algunas columnas tipo object: term, emp_title, url, title, zip_code y addr_state.\n",
    "\n",
    "Estas columnas no se eliminaron en los pasos anteriores porque algunas requerían transformaciones específicas para ser útiles en el modelo, mientras que otras podían eliminarse para mantener interpretabilidad y eficiencia:\n",
    "\n",
    "- term: contiene la duración del préstamo como texto (ej. \"36 months\"). Se convirtió a un valor numérico en meses para que el modelo pueda utilizarlo directamente.\n",
    "\n",
    "- emp_title, url, title: columnas de texto libre con demasiadas categorías únicas y sin estructura clara. Se eliminaron para simplificar el modelo y mantener su interpretabilidad.\n",
    "\n",
    "- zip_code: originalmente un código postal completo, se redujo a los primeros 3 dígitos y se codificó numéricamente mediante OrdinalEncoder, manejando correctamente los códigos nuevos que aparezcan en el conjunto de prueba. Esto permite conservar información geográfica sin explotar la dimensionalidad.\n",
    "\n",
    "- addr_state: contiene el estado de residencia. Como tiene pocas categorías, se codificó mediante One-hot con drop='first', generando columnas independientes que el modelo puede interpretar sin introducir redundancia.\n",
    "\n",
    "Esta revisión garantiza que todas las variables sean numéricas o codificadas correctamente, evitando errores al entrenar una regresión logística y manteniendo la interpretabilidad y eficiencia del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Convertir 'term' a número de meses (funciona si ya es int o si es string)\n",
    "# -----------------------------\n",
    "X_train['term'] = X_train['term'].astype(str).str.replace(' months','').astype(int)\n",
    "X_test['term'] = X_test['term'].astype(str).str.replace(' months','').astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Eliminar columnas de texto libre irrelevantes\n",
    "# -----------------------------\n",
    "cols_to_drop = ['emp_title', 'url', 'title', 'desc']\n",
    "X_train = X_train.drop(columns=[c for c in cols_to_drop if c in X_train.columns])\n",
    "X_test = X_test.drop(columns=[c for c in cols_to_drop if c in X_test.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Manejo de zip_code (solo 3 primeros dígitos)\n",
    "# -----------------------------\n",
    "X_train['zip_code'] = X_train['zip_code'].astype(str).str[:3]\n",
    "X_test['zip_code'] = X_test['zip_code'].astype(str).str[:3]\n",
    "\n",
    "ord_enc_zip = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train[['zip_code']] = ord_enc_zip.fit_transform(X_train[['zip_code']])\n",
    "X_test[['zip_code']] = ord_enc_zip.transform(X_test[['zip_code']])\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ One-hot encoding de addr_state (pocas categorías)\n",
    "# -----------------------------\n",
    "if 'addr_state' in X_train.columns:\n",
    "    ohe_state = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "    ohe_state.fit(X_train[['addr_state']])\n",
    "\n",
    "    X_train_state = pd.DataFrame(ohe_state.transform(X_train[['addr_state']]),\n",
    "                                 columns=ohe_state.get_feature_names_out(['addr_state']),\n",
    "                                 index=X_train.index)\n",
    "\n",
    "    X_test_state = pd.DataFrame(ohe_state.transform(X_test[['addr_state']]),\n",
    "                                columns=ohe_state.get_feature_names_out(['addr_state']),\n",
    "                                index=X_test.index)\n",
    "\n",
    "    X_train = X_train.drop(columns=['addr_state']).join(X_train_state)\n",
    "    X_test = X_test.drop(columns=['addr_state']).join(X_test_state)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Convertir columnas datetime a métricas numéricas\n",
    "# -----------------------------\n",
    "date_cols = ['earliest_cr_line', 'issue_d', 'last_pymnt_d',\n",
    "             'next_pymnt_d', 'last_credit_pull_d',\n",
    "             'payment_plan_start_date', 'debt_settlement_flag_date']\n",
    "\n",
    "for col in date_cols:\n",
    "    if col in X_train.columns:\n",
    "        # Convertir a datetime\n",
    "        X_train[col] = pd.to_datetime(X_train[col], errors='coerce')\n",
    "        X_test[col] = pd.to_datetime(X_test[col], errors='coerce')\n",
    "        \n",
    "        # Métrica numérica: días desde hoy\n",
    "        X_train[col + '_days_since'] = (pd.to_datetime('today') - X_train[col]).dt.days\n",
    "        X_test[col + '_days_since'] = (pd.to_datetime('today') - X_test[col]).dt.days\n",
    "\n",
    "# Eliminar columnas datetime originales\n",
    "X_train = X_train.drop(columns=[c for c in date_cols if c in X_train.columns])\n",
    "X_test = X_test.drop(columns=[c for c in date_cols if c in X_test.columns])\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Verificación final: no object ni datetime\n",
    "# -----------------------------\n",
    "print(\"Tipos de columnas finales X_train:\\n\", X_train.dtypes.value_counts())\n",
    "print(\"Columnas tipo object restantes:\", X_train.select_dtypes(include='object').columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb35c3f",
   "metadata": {},
   "source": [
    "Tras la codificación de las columnas tipo object y la eliminación de las columnas de texto libre irrelevantes, el dataset pasó de tener 94 columnas numéricas y 26 columnas tipo object, a:\n",
    "\n",
    "215 columnas de tipo float64\n",
    "\n",
    "4 columnas de tipo int64\n",
    "\n",
    "Esto refleja que todas las variables categóricas han sido correctamente codificadas:\n",
    "\n",
    "- Las variables binarias y ordinales se mantienen como numéricas (int64 o float64).\n",
    "\n",
    "- Las variables categóricas con pocas categorías, como addr_state, se codificaron con One-hot.\n",
    "\n",
    "- Las columnas de fecha se transformaron a métricas numéricas (días transcurridos desde cada fecha), permitiendo que los modelos interpreten la información temporal.\n",
    "\n",
    "- Las variables de texto libre que no aportaban información estructurada se eliminaron (emp_title, url, title, desc).\n",
    "\n",
    "Como resultado, no quedan columnas tipo object ni datetime, asegurando que el dataset esté completamente listo para entrenar modelos de regresión logística, IsolationForest o RandomForest, evitando errores y manteniendo interpretabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14277c",
   "metadata": {},
   "source": [
    "### Codificación del Target (loan_status) a Binario\n",
    "\n",
    "El objetivo del proyecto es predecir si un préstamo caerá en default o no. Originalmente, la columna loan_status contenía múltiples estados textuales como:\n",
    "\n",
    "'Charged Off', 'Fully Paid', 'Current', 'Late (16-30 days)', 'Late (31-120 days)', \n",
    "'Does not meet the credit policy. Status:Charged Off', 'Does not meet the credit policy. Status:Fully Paid', 'In Grace Period'\n",
    "\n",
    "\n",
    "Para simplificar el problema a clasificación binaria, se realizó la siguiente transformación:\n",
    "\n",
    "Se definieron ciertos estados como default (1):\n",
    "'Charged Off', 'Does not meet the credit policy. Status:Charged Off', 'Does not meet the credit policy. Status:Fully Paid', 'Late (16-30 days)', 'Late (31-120 days)'.\n",
    "\n",
    "Todos los demás estados se consideraron No Default (0): 'Fully Paid', 'Current', 'In Grace Period'.\n",
    "\n",
    "Esta codificación asegura que el target sea numérico y binario, compatible con modelos de clasificación como RandomForest, XGBoost o regresión logística, evitando errores por valores categóricos y manteniendo la interpretabilidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6bf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Codificar target a binario usando map\n",
    "# -----------------------------\n",
    "default_labels = {\n",
    "    'Charged Off': 1,\n",
    "    'Does not meet the credit policy. Status:Charged Off': 1,\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 1,\n",
    "    'Late (16-30 days)': 1,\n",
    "    'Late (31-120 days)': 1,\n",
    "    'Fully Paid': 0,\n",
    "    'Current': 0,\n",
    "    'In Grace Period': 0  # puedes ajustar según tu criterio\n",
    "}\n",
    "\n",
    "y_train = y_train.map(default_labels)\n",
    "y_test = y_test.map(default_labels)\n",
    "\n",
    "# Revisar conteo de clases\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59011e3",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = IsolationForest(random_state=123)\n",
    "df_num = X_train.copy()\n",
    "df_num['outlier_flag'] = iso.fit_predict(X_train)\n",
    "df_num['outlier_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia del dataset de entrenamiento\n",
    "\n",
    "df_train_clean = X_train.copy()\n",
    "\n",
    "#  Aplicar IsolationForest para detectar outliers\n",
    "iso = IsolationForest(random_state=123, contamination='auto')\n",
    "df_train_clean['outlier_flag'] = iso.fit_predict(df_train_clean)\n",
    "\n",
    "\n",
    "# Eliminar registros considerados outliers (-1)\n",
    "\n",
    "df_train_clean = df_train_clean[df_train_clean['outlier_flag'] == 1]\n",
    "\n",
    "\n",
    "# Eliminar la columna outlier_flag si no se va a usar como característica\n",
    "df_train_clean = df_train_clean.drop(columns=['outlier_flag'])\n",
    "y_train_clean = y_train.loc[df_train_clean.index]\n",
    "\n",
    "print(\"Número de registros después de eliminar outliers:\", df_train_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a651f",
   "metadata": {},
   "source": [
    "Se aplicó IsolationForest para identificar registros atípicos en el dataset de entrenamiento. Los outliers son puntos que presentan patrones muy diferentes al resto de los datos y podrían distorsionar los resultados de modelos sensibles, como la regresión logística.\n",
    "\n",
    "Tras la detección, se eliminaron los 45 registros considerados outliers de un total de 153,846, lo que representa menos del 0.03% del dataset. Esta eliminación asegura que el modelo se entrene sobre datos consistentes, manteniendo la interpretabilidad y evitando que valores extremos afecten los coeficientes.\n",
    "\n",
    "El dataset resultante conserva prácticamente toda la información original, pero más “limpio”, garantizando una base sólida para el entrenamiento de modelos de regresión y otros algoritmos supervisados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3dcec",
   "metadata": {},
   "source": [
    "# Seleccion de caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68316724",
   "metadata": {},
   "source": [
    "## Método 1 Feature importance de RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de RF\n",
    "model = RandomForestClassifier(random_state=42,n_jobs=-1).fit(df_train_clean, y_train_clean)\n",
    "# Importancia de características\n",
    "importances = model.feature_importances_/model.feature_importances_.sum()*100\n",
    "# Convertir a DataFrame\n",
    "df_rf_imp = pd.DataFrame({'feature': df_train_clean.columns,'rf_importance': importances}).sort_values(by='rf_importance', ascending=False)\n",
    "# Calculamos la importancia acumulada\n",
    "df_rf_imp['rf_importance_acum'] = df_rf_imp['rf_importance'].cumsum()\n",
    "df_rf_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ce962",
   "metadata": {},
   "source": [
    "## Metodo 2 Permutation/Shuffle importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e512fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para esta técnica y la de shap se necesita conjunto de validación\n",
    "X_train1, X_val, y_train1, y_val = train_test_split(df_train_clean, y_train_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajustamos el modelo\n",
    "model_xgb = XGBClassifier(objective='binary:logistic',random_state=42,use_label_encoder=False,eval_metric='logloss').fit(X_train1, y_train1)\n",
    "\n",
    "# Realizamos 10 permutaciones por cada característica (se usa neg_mean_absolute_error)\n",
    "perm = permutation_importance(model_xgb, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "df_perm_imp = pd.DataFrame({'feature': df_train_clean.columns, 'perm_imp': perm.importances_mean*100}).sort_values('perm_imp', ascending=False)\n",
    "df_perm_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b7091",
   "metadata": {},
   "source": [
    "## Metodo 3 SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "model_lgbm = lgb.LGBMClassifier(random_state=42, n_jobs=-1).fit(df_train_clean, y_train)\n",
    "\n",
    "explainer = shap.Explainer(model_lgbm, X_val)   # usa el mismo X_val\n",
    "shap_vals = explainer(X_val).values\n",
    "\n",
    "imp_shap = np.abs(shap_vals).mean(axis=0)\n",
    "imp_shap_pct = imp_shap/imp_shap.sum()*100\n",
    "df_shap_imp = pd.DataFrame({\"feature\": X_val.columns, \"shap_imp\": imp_shap_pct}).sort_values('shap_imp', ascending=False)\n",
    "df_shap_imp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
