{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea46098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ccea56",
   "metadata": {},
   "source": [
    "# Cargamos una base de datos SQLite (Tabla \"loans\")\n",
    "\n",
    "En este bloque de código se realizó la carga de los datos del archivo CSV en la base de datos SQLite de manera eficiente, utilizando la técnica de lectura por fragmentos (chunks) para manejar los 2 millones de registros sin saturar la memoria. Cada fragmento de 50.000 filas se lee con pandas.read_csv y se convierte en una tabla SQL mediante to_sql, creando automáticamente la tabla \"loans\" en la base de datos si aún no existía y agregando los registros en cada iteración. De esta forma, se logró almacenar toda la información de manera estructurada en SQLite, conservando todas las columnas del dataset original y permitiendo consultas posteriores sin necesidad de cargar el archivo completo en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72fa3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(r\"C:\\Users\\anaga\\Documents\\final_project_creditscoring\\Data\\credit_scoring.db\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "073a8628",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m chunksize = \u001b[32m50000\u001b[39m  \u001b[38;5;66;03m# Ajusta según memoria disponible\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43managa\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDocuments\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mfinal_project_creditscoring\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mData\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43maccepted_2007_to_2018Q4.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43missue_d\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# chunk = chunk[(chunk['issue_d'] >= '2010-01-01') & (chunk['issue_d'] < '2018-01-01')]\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Guardar en SQLite, creando la tabla automáticamente con todas las columnas\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloans\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[39m, in \u001b[36mTextFileReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame:\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1845\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[39m, in \u001b[36mTextFileReader.get_chunk\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m   1983\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1984\u001b[39m     size = \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m.nrows - \u001b[38;5;28mself\u001b[39m._currow)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:850\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:921\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1083\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._convert_column_data\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1456\u001b[39m, in \u001b[36mpandas._libs.parsers._maybe_upcast\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_core\\multiarray.py:1156\u001b[39m, in \u001b[36mputmask\u001b[39m\u001b[34m(a, mask, values)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1109\u001b[39m \u001b[33;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[32m   1110\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1151\u001b[39m \n\u001b[32m   1152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath.putmask)\n\u001b[32m   1157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mputmask\u001b[39m(a, /, mask, values):\n\u001b[32m   1158\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1159\u001b[39m \u001b[33;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m \n\u001b[32m   1198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "chunksize = 50000  # Ajusta según memoria disponible\n",
    "\n",
    "for chunk in pd.read_csv(r\"C:\\Users\\anaga\\Documents\\final_project_creditscoring\\Data\\accepted_2007_to_2018Q4.csv\", chunksize=chunksize, parse_dates=['issue_d']):\n",
    " \n",
    "    # chunk = chunk[(chunk['issue_d'] >= '2010-01-01') & (chunk['issue_d'] < '2018-01-01')]\n",
    "    \n",
    "    # Guardar en SQLite, creando la tabla automáticamente con todas las columnas\n",
    "    chunk.to_sql(\"loans\", conn, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fd4d0",
   "metadata": {},
   "source": [
    "En el siguiente bloque de código tiene como objetivo crear un dataset de muestra balanceado por mes a partir de la base de datos completa loans. Primero, se cuentan los registros de cada mes usando SQLite para identificar los meses que tienen al menos un mínimo de datos (MIN_REGISTROS_POR_MES). Luego, se calcula cuántos registros tomar de cada mes para que el dataset final tenga un total aproximado de TOTAL_REGISTROS. Después, se realiza un muestreo aleatorio por mes directamente desde la base de datos, asegurando que cada mes seleccionado tenga la misma cantidad de registros y que se excluyan los meses con pocos datos. Finalmente, se concatenan todos los fragmentos en un único DataFrame df_final, se convierten las fechas a formato datetime y se verifica que la forma del dataset y la cantidad de meses únicos sean correctas. Esto permite trabajar con un dataset más manejable y representativo de todos los meses con datos suficientes, sin cargar los 2 millones de registros originales en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81fa722b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m MIN_REGISTROS_POR_MES = \u001b[32m1000\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1️⃣ Obtener número de registros por mes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m monthly_counts = \u001b[43mpd\u001b[49m.read_sql(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m    SELECT\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m        strftime(\u001b[39m\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, issue_d) AS year_month,\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m        COUNT(*) AS n_registros\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m    FROM loans\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m    GROUP BY year_month\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33m\"\"\"\u001b[39m, conn)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 2️⃣ Filtrar meses con suficientes datos\u001b[39;00m\n\u001b[32m     14\u001b[39m valid_months = monthly_counts[\n\u001b[32m     15\u001b[39m     monthly_counts[\u001b[33m'\u001b[39m\u001b[33mn_registros\u001b[39m\u001b[33m'\u001b[39m] >= MIN_REGISTROS_POR_MES\n\u001b[32m     16\u001b[39m ][\u001b[33m'\u001b[39m\u001b[33myear_month\u001b[39m\u001b[33m'\u001b[39m].tolist()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "TOTAL_REGISTROS = 200_000\n",
    "MIN_REGISTROS_POR_MES = 1000\n",
    "\n",
    "# 1️⃣ Obtener número de registros por mes\n",
    "monthly_counts = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        strftime('%Y-%m', issue_d) AS year_month,\n",
    "        COUNT(*) AS n_registros\n",
    "    FROM loans\n",
    "    GROUP BY year_month\n",
    "\"\"\", conn)\n",
    "\n",
    "# 2️⃣ Filtrar meses con suficientes datos\n",
    "valid_months = monthly_counts[\n",
    "    monthly_counts['n_registros'] >= MIN_REGISTROS_POR_MES\n",
    "]['year_month'].tolist()\n",
    "\n",
    "# 3️⃣ Calcular cuántos registros tomar por mes\n",
    "rows_per_month = TOTAL_REGISTROS // len(valid_months)\n",
    "\n",
    "print(f\"Meses válidos: {len(valid_months)}\")\n",
    "print(f\"Registros por mes: {rows_per_month}\")\n",
    "\n",
    "# 4️⃣ Muestreo balanceado por mes desde SQLite\n",
    "df_list = []\n",
    "\n",
    "for m in valid_months:\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM loans\n",
    "    WHERE strftime('%Y-%m', issue_d) = '{m}'\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT {rows_per_month}\n",
    "    \"\"\"\n",
    "    df_month = pd.read_sql_query(query, conn)\n",
    "    df_list.append(df_month)\n",
    "\n",
    "# 5️⃣ Dataset final\n",
    "df_final = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 6️⃣ Conversión de fecha\n",
    "df_final['issue_d'] = pd.to_datetime(df_final['issue_d'])\n",
    "\n",
    "\n",
    "# 7️⃣ Verificación\n",
    "print(\"Shape final:\", df_final.shape)\n",
    "print(\"Meses únicos:\", df_final['issue_d'].dt.to_period('M').nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8311804",
   "metadata": {},
   "source": [
    "Confirmamos si hay meses faltantes y si los datos son continuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54850c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No hay meses faltantes. La serie es continua.\n"
     ]
    }
   ],
   "source": [
    "# Obtener meses únicos ordenados\n",
    "meses = (\n",
    "    df_final['issue_d']\n",
    "    .dt.to_period('M')\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "# Convertir a índice temporal\n",
    "meses = pd.PeriodIndex(meses, freq='M')\n",
    "\n",
    "# Crear rango completo esperado\n",
    "rango_completo = pd.period_range(\n",
    "    start=meses.min(),\n",
    "    end=meses.max(),\n",
    "    freq='M'\n",
    ")\n",
    "\n",
    "# Detectar meses faltantes\n",
    "meses_faltantes = rango_completo.difference(meses)\n",
    "\n",
    "# Resultado\n",
    "if len(meses_faltantes) == 0:\n",
    "    print(\"✅ No hay meses faltantes. La serie es continua.\")\n",
    "else:\n",
    "    print(\"⚠️ Hay meses sin datos:\")\n",
    "    print(meses_faltantes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a245f",
   "metadata": {},
   "source": [
    "# Guardar una tabla llamada 'main_table' en nuestro archivo credit_scoring.db\n",
    "Por ultimo guardamos en una tabla el data set anteriormente creado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29453bcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_final\u001b[49m.to_sql(\u001b[33m\"\u001b[39m\u001b[33mmain_table\u001b[39m\u001b[33m\"\u001b[39m, conn, if_exists=\u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      3\u001b[39m conn.close()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_final' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df_final.to_sql(\"main_table\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
